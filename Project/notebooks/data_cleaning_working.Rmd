---
title: "`Project/notebooks/data_cleaning.R`"
author: "TECK Squad"
date: "December 12, 2025"
output:
  rmdformats::readthedown:
    toc_depth: 3        
    code_folding: hide
fontsize: 11pt
geometry: margin=1in
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
# ---- Setup ----
suppressPackageStartupMessages({
  library(tidyverse)
  library(arrow)
  library(dplyr)
  library(stringr)
  #library(rprojroot)
  library(knitr)
  library(kableExtra)
  
  #library(broom)
  #library(car)
  #library(latex2exp)
  #library(MPV)
  #library(glmnet)
})

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 6.5,
  fig.height = 4
)
theme_set(theme_bw(base_size = 12))

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# ---- Helper functions ----
fmt   <- function(x, d = 3)
  round(x, d)
fmt_e <-
  function(x, digits = 3)
    formatC(x, format = "e", digits = digits)

# ---- Load data I/O utilities and raw data ----
source("Project/scripts/data_io.R")
# Raw data from parquet via `data_io.R`
ad_raw <- load_ad_data(preview = FALSE)

# ---- Data Log setup ----
cleaning_log <- tibble::tibble(
  step_id = character(),
  variable = character(),
  issue = character(),
  decision = character(),
  rows_affected = integer()
)

log_change <- function(step_id, variable, issue, decision, rows_affected = NA_integer_) {
  cleaning_log <<- cleaning_log %>%
    add_row(
      step_id       = step_id,
      variable      = variable,
      issue         = issue,
      decision      = decision,
      rows_affected = rows_affected
    )
}
```

---

# **Purpose**

This notebook documents the full data-cleaning workflow for the Oregon ad bidding dataset.

  - **Input:** raw parquet data loaded via `load_ad_data()` (see `Project/scripts/data_io.R`).
  - **Output:** a cleaned analysis-ready dataset saved under `Project/data/...`.

---

---

# **DATA OVERVIEW**

- According to the data dictionary, each row represents **one bidder’s response for one requested size within a single ad auction**, meaning:
  - Each `AUCTION_ID` appears multiple times  
  - There may be several responses for the same ad size within the same auction  

---

## Dimensions and Structure

We begin with a high-level look at the raw dataset.

```{r overview-dims, message=FALSE, 'hold'=TRUE}
# Dimensions
n_rows <- nrow(ad_raw)
n_cols <- ncol(ad_raw)

dim_table <- tibble(
  Metric = c("Number of Rows", "Number of Columns"),
             Value = c(n_rows, n_cols)
)

dim_table %>%
  kable(caption = "Dataset Dimentions") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

---

## Basic summaries

We summarize all variables numerically and inspect missingness patterns using the helper summary function defined in `data_io.R`.

```{r overview-summary}
summarize_ad_data(ad_raw)
```

The only variables with missing data are `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP`, with **21,198 missing rows** each.

---

## Expected vs. Actual Data Types

We can compare expected data types as given by the data dictionary to the actual type of each variable in the data:

```{r overview-expected-vs-actual, message=FALSE}
expected_types <- tibble(
  Variable = c(
    "TIMESTAMP",
    "DATE_UTC",
    "AUCTION_ID",
    "PUBLISHER_ID",
    "DEVICE_TYPE",
    "DEVICE_GEO_COUNTRY",
    "DEVICE_GEO_REGION",
    "DEVICE_GEO_CITY",
    "DEVICE_GEO_ZIP",
    "DEVICE_GEO_LAT",
    "DEVICE_GEO_LONG",
    "REQUESTED_SIZES",
    "SIZE",
    "PRICE",
    "RESPONSE_TIME",
    "BID_WON"
  ), 
  Expected_Class = c(
    "datetime",          # TIMESTAMP_NTZ
    "date",              # DATE
    "categorical",       # Auction ID
    "categorical",       # Publisher ID
    "categorical",       # desktop/mobile/tablet
    "categorical",       # 2-letter country
    "categorical",       # 2-letter USPS region
    "categorical",       # City name
    "categorical",       # ZIP / ZIP+4 string
    "numeric",           # Latitude
    "numeric",           # Longitude
    "categorical",       # List of sizes
    "categorical",       # Single size
    "numeric",           # CPM bid
    "numeric",           # Response time in ms
    "boolean"            # TRUE/FALSE winner
  ),
  Expected_Detail = c(
    "TIMESTAMP_NTZ (UTC event time)",
    "DATE (UTC calendar date)",
    "VARCHAR (auction identifier)",
    "VARCHAR (publisher/site identifier)",
    "VARCHAR, (device category, string)",
    "VARCHAR(2) (ISO 2-letter country code)",
    "VARCHAR(2) (USPS state/region postal code)",
    "VARCHAR, (city name, string)",
    "VARCHAR(10) (ZIP or ZIP+4)",
    "FLOAT (latitude of device)",
    "FLOAT (longitude of device)",
    "VARCHAR/ARRAY (all requested sizes, ex: '300x250, 320x50')",
    "VARCHAR (size for this bid, ex: `300x250')",
    "NUMBER(12,6) (CPM bid in USD)",
    "NUMBER(10,0) (bidder latency in ms)",
    "BOOLEAN (if bid won, TRUE/FALSE)"
  )
)

expected_types <- expected_types %>%
  arrange(factor(
    Expected_Class,
    levels = c("numeric", "categorical", "datetime", "date", "boolean")
  ))

actual_types <- tibble(
  Variable = names(ad_raw),
  Actual_Type = sapply(ad_raw, class)
)

comparison <- expected_types %>%
  left_join(actual_types, by = "Variable")

comparison %>%
  kable(caption = "Expected vs Actual Types (Data Dictionary vs Raw Data)") %>%
    kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

Many of these are suspicious or inconsistent with the data dictionary:
  
  - **`PRICE`** and **`RESPONSE_TIME`** 
      - Expected *numeric* $\to$ Stored as `character`
      - Likely due to *non-numeric characters* or inconsistent formatting.
  - **`DEVICE_TYPE`**
      - Expected descriptive *string* $\to$ Stored as an `integer` code
      - Implies an encoded representation without a visible lookup table.
  - **`DEVICE_GEO_COUNTRY`** 
      - Expected per dictionary $\to$ Not present in this data subset
  - **`TIMESTAMP`** and **`DATE_UTC`** 
      - Expected *datetime/date* $\to$ Stored as `character`
      - Will need to be converted prior to any time-based analysis.
  - **`BID_WON`** 
      - Expected *boolean* $to$ Stored as `character`
      - Will require re-coding to `logical` (TRUE/FALSE) type.

In addition, some geographic and categorical fields (**`DEVICE_GEO_REGION`**, **`DEVICE_GEO_CITY`**, and **`DEVICE_GEO_ZIP`**) appear broadly consistent with expected classes, but will need to be fully analysed for any messy real-world values such as **inconsistent casing**, **invalid state codes**, or **irregular ZIP formats**. These issues will be fully examined and addressed in the variable-level cleaning sections that follow.

---

---

# **FULL DATASET**

This section examines the dataset at the row level to ensure there are no duplicated records and to understand whether the available columns can uniquely identify each row. According to the data dictionary, the true grain of the full dataset is:

> **(AUCTION_ID, bidder/seat, SIZE, \dots)**

However, bidder- and seat-level identifiers are **not included** in this subset, so we do not expect any single column or simple composite key to uniquely identify each record.

---

## Duplicated rows

We test for full-row duplicates.

```{r full-duplicates, results='hold'}
# Count total rows and distinct rows across all columns
n_raw      <- nrow(ad_raw)
n_distinct <- ad_raw %>% distinct() %>% nrow()
n_dups     <- n_raw - n_distinct

cat("Total rows:                       ", n_raw, "\n")
cat("Distinct rows (all columns):      ", n_distinct, "\n")
cat("Exact duplicate rows (by count):  ", n_dups, "\n")
```

**Decision:** Drop exact duplicates, while keeping one copy of each unique row.

We implement our decision for handling full-row duplicates and add to data log:

```{r full-dup-clean, results='hold'}
# Remove exact full-row duplicates (keep one copy)
ad_clean <- ad_raw %>%  distinct()

log_change(
    step_id = "G1",
    variable = "ALL",
    issue = "Exact duplicate rows present",
    decision = "Dropped full duplicate rows keeping one copy.",
    rows_affected = n_dups
  )
```

---

## Keys

Since bidder/seat identifiers are missing, no single column is expected to uniquely identify a record.
We begin by testing the most intuitive composite key:

> (`AUCTION_ID`, `SIZE`, `PRICE`)

```{r key-check-intial}
simple_unique <- ad_clean %>%
  distinct(AUCTION_ID, SIZE, PRICE) %>%
  nrow()

cat("Unique composite (AUCTION_ID, SIZE, PRICE): ", simple_unique)
```

The dataset contains `r nrow(ad_clean)` rows after duplicate removal,  
but only `r simple_unique` unique combinations of (`AUCTION_ID`, `SIZE`, `PRICE`).  
Thus, this composite key does **not** uniquely identify rows.

We create a helper function to allow us to search for a combination which will work as a key. 

**NOTE:** To avoid computational blow-up, we focus only on the most likely variables and limit our number of variables per composite key to 4:

```{r composite-key-helper}
check_candidate_keys <- function(data, cols, max_size = 4, show_full = TRUE) {
  results <- list()
  idx <- 1L
  
  for (k in seq_len(max_size)) {
    combos <- combn(cols, k, simplify = FALSE)
    
    for (combo in combos) {
      n_unique <- data %>%
        distinct(across(all_of(combo))) %>%
        nrow()
      
      results[[idx]] <- tibble(
        combo   = paste(combo, collapse = " + "),
        size    = k,
        n_rows  = nrow(data),
        n_unique = n_unique,
        is_key  = (n_unique == nrow(data))
      )
      idx <- idx + 1L
    }
  }
  
  results <- bind_rows(results)
  
  if (show_full) {
    # Full table (shown if needed for further exploration)
    results %>%
      arrange(desc(is_key), size, desc(n_unique))
  } else {
    # Compact summary
    results %>%
      group_by(size) %>%
      summarise(
        num_combos   = n(),
        num_keys     = sum(is_key),
        max_n_unique = max(n_unique),
        .groups = "drop"
      ) %>%
      arrange(size)
  }
}
```

We then use this function to check the most likely candidate variables:

```{r composite-key-search}
composite_candidates <- c(
  "AUCTION_ID",
  "SIZE",
  "PRICE",
  "RESPONSE_TIME",
  "BID_WON",
  "PUBLISHER_ID",
  "DEVICE_TYPE",
  "DEVICE_GEO_CITY",
  "DEVICE_GEO_ZIP",
  "DEVICE_GEO_LAT",
  "DEVICE_GEO_LONG",
  "TIMESTAMP"
)

#key_results <- check_candidate_keys(
#  data     = ad_clean,
#  cols     = composite_candidates,
#  max_size = 4,
#  show_full = FALSE
#)

#key_results
```

From these results, we can see that no combination (up to four variables) of these candidate variables will create a composite key to uniquely identify the rows of this dataset.

---

## Summary

  - Exact duplicate rows were removed (`r n_dups` rows).
  - No reasonable variable or combination of variables uniquely identifies each row.  
  This is consistent with the data dictionary, which indicates that the true grain requires `bidder/seat`-level fields not present in this subset.

We proceed using **`ad_clean`** as our working dataset for variable-level cleaning.

---

---

# **VARIABLE-LEVEL CLEANING**

The following sections clean each variable (or group of related variables) in an order that follows the data dictionary.  
For each variable, we document:

  - The expected type and meaning  
  - Exploration of raw values  
  - Cleaning decisions  
  - Updates to the data-cleaning log  
  - A post-clean summary  

---

## **TIME VARIABLES:** `TIMESTAMP`, `DATE_UTC`

### Description

- `TIMESTAMP`  
  - **Type (dictionary):** `TIMESTAMP_NTZ`  
  - **Meaning:** Event time (UTC) when the log row was recorded 
  - **Expectation:** A full datetime (date + time), treated as UTC
  - **Actual Type:** `character`
  - **Missing:** None

- `DATE_UTC`  
  - **Type (dictionary):** `DATE`  
  - **Meaning:** Calendar date (UTC) used for partitioning / grouping
  - **Expectation:** A date consistent with the date portion of `TIMESTAMP`
  - **Actual Type:** `character`
  - **Missing:** None

### Exploration

Sample a few `TIMESTAMP` values:
```{r timestamp-sample}
set.seed(42)
sample(ad_clean$TIMESTAMP, size = 20)
```


```{r timestamp-count-format, results='hold'}
dash_rows       <- str_detect(ad_clean$TIMESTAMP, "-") %>%  sum()
slash_rows      <- str_detect(ad_clean$TIMESTAMP, "/") %>%  sum()
na_date_rows    <- str_detect(ad_clean$TIMESTAMP, "^NA") %>%  sum()

cat("Rows with '-' in date portion:   ", dash_rows,  "\n")
cat("Rows with '/' in date portion:   ", slash_rows, "\n")
cat("Rows starting with 'NA ' date:   ", na_date_rows, "\n")
cat("These three account for all rows:", 
    dash_rows + slash_rows + na_date_rows == nrow(ad_clean))
```

**Observations**

  - `r dash_rows` of `TIMESTAMP` values use a dash-separated date (e.g., `"2025-10-12 14:03:27"`).
  - `r slash_rows` uses a slash-separated format (e.g., `"10/12/2025 14:03:27"`).  
  - `r na_date_rows` appear with an `NA` date and a valid time-of-day (e.g., `"NA 14:03:27"`).  
  - The raw columns themselves do **not** contain missing values: all rows have some string in `TIMESTAMP` and `DATE_UTC`. 

**Decisions:**

  - Recover missing dates in `TIMESTAMP` via `DATE_UTC`
  - Parse `TIMESTAMP` into a POSIXct (UTC) variable with multiple formats.
  - Convert `DATE_UTC` to *Date* type, keeping raw string as-is.
  
### Cleaning

Recover `TIMESTAMP` where the date part was missing but time-of-day exists, using `DATE_UTC` as the date source:

```{r timestamp-recover-date}
# Extract date and time components
ad_clean <- ad_clean %>%
  mutate(
    ts_date_raw = str_extract(TIMESTAMP, "^[^ ]+"),
    ts_time_raw = str_extract(TIMESTAMP, "\\d{1,2}:\\d{1,2}:\\d{1,2}")
  )

# Fix rows where the date is 'NA', keeping all others as-is
ad_clean <- ad_clean %>%
  mutate(TIMESTAMP_fixed = case_when(
    ts_date_raw == "NA" & !is.na(DATE_UTC) & !is.na(ts_time_raw) ~
      paste(DATE_UTC, ts_time_raw),
    TRUE ~ TIMESTAMP
  ))
```

Parse the fixed `TIMESTAMP` into POSIXct:

```{r timestamp-parse}
ad_clean <- ad_clean %>%
  mutate(TIMESTAMP_clean = lubridate::parse_date_time(
    TIMESTAMP_fixed,
    orders = c("ymd HMS", "mdy HMS"),
    tz = "UTC"
  ))

n_na_ts <- sum(is.na(ad_clean$TIMESTAMP_clean))
cat("Rows with NA in TIMESTAMP_clean after parsing: ", n_na_ts, "\n")
```

Convert `DATE_UTC` to Date:

```{r date_utc-to-date}
ad_clean <- ad_clean %>%
  mutate(DATE_UTC_clean = as.Date(DATE_UTC))

cat("Rows with NA in DATE_UTC_clean: ", sum(is.na(ad_clean$DATE_UTC_clean)), "\n")
```

Log entries for time variables:

```{r time-log}
log_change(
  step_id = "T1",
  variable = "TIMESTAMP",
  issue = "Inconsistent TIMESTAMP formats, including 'NA' date tokens.",
  decision = "Recover 'NA' dates using `DATE_UTC`; parsed to POSIXct (UTC).",
  rows_affected = nrow(ad_clean)
)

log_change(
  step_id = "T2",
  variable = "DATE_UTC",
  issue = "DATE_UTC stored as character.",
  decision = "Converted to Date type.",
  rows_affected = nrow(ad_clean)
)
```

### Summary

  - `TIMESTAMP`: 
      - Inconsistently formatted (mixture of dash- vs. slash-separated dates and a small number of entries with an `NA` date token).
      - We repaired rows with an `NA` date by using the date from `DATE_UTC`, then parsed the corrected string into `TIMESTAMP_clean` (POSIXct, UTC).  

  - `DATE_UTC`:
      - Converted from character to `DATE_UTC_clean` (Date type).  
      
  - No rows were dropped as part of Time Variables cleaning steps.

---

## **IDENTIFIERS:** `AUCTION_ID`, `PUBLISHER_ID`

### Description

- `AUCTION_ID`  
  - **Type (dictionary):** `VARCHAR`  
  - **Meaning:** Unique identifier for a single auction.  
  - **Expectation:** Repeated across multiple rows (one auction $to$ many bid lines).  
  - **Actual Type:** `character`  
  - **Missing:** None  

- `PUBLISHER_ID`  
  - **Type (dictionary):** `VARCHAR`  
  - **Meaning:** Identifier for the publisher / site / app running the auction.  
  - **Expectation:** Medium/high cardinality; used for grouping and filtering.  
  - **Actual Type:** `character`  
  - **Missing:** None  

### Exploration

These are opaque identifiers:

  - We do **not** attempt to interpret or recode their values
  - We only ensure they are well-formed and structurally consistent with the data dictionary.

We begin with basic uniqueness and format checks.

```{r ids-basic-explore, results='hold'}
ad_clean %>%
  summarise(
    n_rows                 = n(),
    n_auctions             = n_distinct(AUCTION_ID),
    n_publishers           = n_distinct(PUBLISHER_ID),
    avg_rows_per_auction   = n_rows / n_auctions,
    avg_rows_per_publisher = n_rows / n_publishers
  )
```

Check for leading/trailing whitespace:

```{r ids-whitespace-missing, results='hold'}
auct_ws <- sum(str_detect(ad_clean$AUCTION_ID, "^\\s|\\s$"))
pub_ws  <- sum(str_detect(ad_clean$PUBLISHER_ID, "^\\s|\\s$"))

cat("AUCTION_ID rows with whitespace:  ", auct_ws, "\n")
cat("PUBLISHER_ID rows with whitespace:", pub_ws,  "\n")
```

  - All identifier fields appear to be free of whitespace issues.

The client clarified that an auction is expected to have **exactly one** winning bid. 
We evaluate this assumption:

```{r ids-auction-outcome, results='hold'}
auction_win_summary <- ad_clean %>%
  group_by(AUCTION_ID) %>%
  summarise(
    n_rows = n(),
    n_won  = sum(BID_WON == "TRUE",  na.rm = TRUE),
    n_lost = sum(BID_WON == "FALSE", na.rm = TRUE),
    .groups = "drop"
  )

# Auctions violating the "exactly one winner" rule
auctions_multiwin <- auction_win_summary %>% filter(n_won > 1)
auctions_zerowin  <- auction_win_summary %>% filter(n_won == 0)

cat("Auctions with >1 winning bid:  ", nrow(auctions_multiwin), "\n")
cat("Auctions with 0 winning bids:  ", nrow(auctions_zerowin),  "\n")
```

We investigate how many rows are affected by these problematic auctions:

```{r problematic-auction-rows, results='hold'}
multiwin_ids <- auctions_multiwin$AUCTION_ID
zerowin_ids  <- auctions_zerowin$AUCTION_ID

multiwin_rows <- ad_clean %>%
  filter(AUCTION_ID %in% multiwin_ids) %>%
  nrow()

zerowin_rows <- ad_clean %>%
  filter(AUCTION_ID %in% zerowin_ids) %>%
  nrow()

cat("Rows in multi-winner auctions: ", multiwin_rows, "\n")
cat("Rows in zero-winner auctions:  ", zerowin_rows,  "\n")
cat("Total problematic rows:        ", multiwin_rows + zerowin_rows, "\n")
```

**Observations**

  - Multi-winner auctions: **`r nrow(auctions_multiwin)` auctions**, **`r multiwin_rows` rows**  
  - Zero-winner auctions: **`r nrow(auctions_zerowin)` auctions**, **`r zerowin_rows` rows**  
  - Combined problematic rows represent  **`r round((multiwin_rows + zerowin_rows) / nrow(ad_clean) * 100, 4)` \%**  
    of the dataset.
  - These cases are **logically impossible** under the client’s business rules and indicate corrupted or incomplete auction records.

**Decision:**  
Since an auction must have *exactly one* winner, correcting the winner is impossible, and the number of rows affected is a very small fraction of the dataset, we remove all auctions where `n_won != 1`.

### Cleaning

Identifier fields (`AUCTION_ID`, `PUBLISHER_ID`) require **minimal cleaning**:

  - No missing values
  - No leading/trailing whitespace
  - Behavior matches expectations

We still create trimmed versions for consistency:

```{r ids-clean, results='hold'}
ad_clean <- ad_clean %>%
  mutate(
    AUCTION_ID_clean   = str_trim(as.character(AUCTION_ID)),
    PUBLISHER_ID_clean = str_trim(as.character(PUBLISHER_ID))
  )
```

We then remove rows belonging to zero-win or multi-win auctions:

```{r ids-remove-invalid-auctions, results='hold'}
# Auctions that violate business logic: n_won != 1
invalid_ids <- auction_win_summary %>%
  filter(n_won != 1) %>%
  pull(AUCTION_ID)

n_invalid_rows <- ad_clean %>%
  filter(AUCTION_ID %in% invalid_ids) %>%
  nrow()

# Remove invalid auctions
ad_clean <- ad_clean %>%
  filter(!AUCTION_ID %in% invalid_ids)

cat("Total rows removed due to invalid auction outcome structure: ",
    n_invalid_rows, "\n")
```

Log entries for identifier variables:

```{r ids-log, results='hold'}
log_change(
  step_id = "I1",
  variable = "AUCTION_ID",
  issue = "Auctions with zero or multiple winners violate business rules.",
  decision = "Removed all rows belonging to auctions where n_won != 1.",
  rows_affected = n_invalid_rows
)
```

### Summary

  - Both identifier fields were already well-formed:
      - No missing values  
      - No whitespace issues  
      - High-cardinality and consistent structure  
  - Clean versions (`AUCTION_ID_clean`, `PUBLISHER_ID_clean`) were created for consistency.
  - Structural validation revealed **invalid auctions** with zero or multiple winners.  
      - These represent impossible events under domain rules, and correcting them is not feasible.  
      - A total of **`r n_invalid_rows`** rows were removed to ensure dataset consistency.  
  - The remaining dataset now contains only auctions with a valid outcome structure.

---

## **DEVICE ATTRIBUTE:** `DEVICE_TYPE`

### Description

- `DEVICE_TYPE`  
  - **Type (dictionary):** `VARCHAR`
  - **Meaning:** Indicates the type of device from which the impression originated  
  - **Expectation:** Categorical values such as "mobile", "desktop", "tablet"  
  - **Actual Type:** `integer` codes
  - **Missing:** None  

A client-provided lookup table defines the meaning of these codes in our subset:

| Code | Description        |
|-----:|--------------------|
| 0    | Mobile/Tablet      |
| 1    | Personal Computer  |
| 2    | Connected TV       |
| 4    | Tablet             |
| 5    | Connected Device   |

---

### Exploration

Check for whitespace or formatting anomalies:

```{r device-type-whitespace}
sum(str_detect(as.character(ad_clean$DEVICE_TYPE), "^\\s|\\s$"))
```

**Observations**

- `DEVICE_TYPE` contains **no missing values**.  
- The column is stored as an **integer**, not a descriptive string.  
- There are **5 unique codes** associated with an accompanying lookup table.
- No leading or trailing whitespace was detected. 
- The team agreed to **retain raw integer codes** and use mapping only for interpretability.


**Decision:** 

  - Keep integer codes as-is and do not recode `DEVICE_TYPE` into descriptive labels.  
  - Apply mapping only in EDA/plots and not in the cleaned dataset.
  

### Cleaning

No transformation of `DEVICE_TYPE` is required. 

Create a pass-through cleaned column for schema consistency:

```{r device-type-clean}
ad_clean <- ad_clean %>%
  mutate(DEVICE_TYPE_clean = DEVICE_TYPE)
```

Because there are **no persistent changes**, **no log entry is added** for this variable.

### Summary

  - `DEVICE_TYPE` is a clean, non-missing integer column representing device categories.
  - No inconsistencies or formatting issues were detected.  
  - The dataset preserves the raw integer representation, while a lookup table will be used for interpretation in downstream analysis.  

---

## **GEOLOCATION FIELDS:** `DEVICE_GEO_REGION`, `DEVICE_GEO_CITY`, `DEVICE_GEO_ZIP`, `DEVICE_GEO_LAT`, `DEVICE_GEO_LONG`

### Description

- `DEVICE_GEO_REGION`  
  - **Type (dictionary):** `categorical` (2-letter USPS state/region code)  
  - **Meaning:** U.S. state or region derived from device IP  
  - **Expectation:** For this dataset, all impressions are from Oregon $\to$ code should be "OR"  
  - **Actual type:** `character`  
  - **Missing:** None

- `DEVICE_GEO_CITY`  
  - **Type (dictionary):** `categorical` (free-text city name)  
  - **Meaning:** City inferred from device IP  
  - **Expectation:** Valid city or locality name within Oregon  
  - **Actual type:** `character`  
  - **Missing:** 21,198 in raw data

- `DEVICE_GEO_ZIP`  
  - **Type (dictionary):** `categorical` (ZIP or ZIP+4)  
  - **Meaning:** ZIP code derived from device IP  
  - **Expectation:** Oregon ZIP codes (5-digit or ZIP+4)  
  - **Actual type:** `character`  
  - **Missing:** 21,198 in raw data

- `DEVICE_GEO_LAT`  
  - **Type (dictionary):** `numeric`  
  - **Meaning:** Latitude of device location  
  - **Expectation:** Within Oregon’s approximate latitude bounds  
  - **Actual type:** `numeric`
  - **Missing:** None

- `DEVICE_GEO_LONG`  
  - **Type (dictionary):** `numeric`  
  - **Meaning:** Longitude of device location  
  - **Expectation:** Within Oregon’s approximate longitude bounds  
  - **Actual type:** `numeric`  
  - **Missing:** None  


### Exploration - `DEVICE_GEO_REGION`

We begin by inspecting distinct region codes and their frequencies:

```{r region-explore}
ad_clean %>%
  dplyr::count(DEVICE_GEO_REGION, sort = TRUE)
```

  - We see four values identified: `"OR"`, `"Or"`, `"oregon"`, and `"xor"`.  
  - Although the first three (`"OR"`, `"Or"`, and `"oregon"`) can easily be justified as meaning Oregon, `"xor"` may mean Oregon or *NOT* Oregon.

We investigate further by looking at the Zip codes of those rows:

```{r region-xor-zips}
# Inspect ZIP prefixes for rows where DEVICE_GEO_REGION looks like "xor"
xor_zip_summary <- ad_clean %>%
  filter(DEVICE_GEO_REGION == "xor") %>%
  mutate(zip_prefix = substr(DEVICE_GEO_ZIP, 1, 2)) %>%
  count(zip_prefix, sort = TRUE)

xor_zip_summary
```

The ZIP prefixes for `"xor"` rows are almost all `"97"` (Oregon), with 687 missing ZIPs and a single sentinel-like value (`-9`).

To strengthen this conclusion, we also check whether the corresponding latitude/longitude values fall within Oregon’s approximate geographic bounds.

```{r region-xor-geo}
# Rough Oregon bounds
oregon_lat_min  <- 42
oregon_lat_max  <- 47
oregon_long_min <- -125
oregon_long_max <- -116


xor_check <- ad_clean %>%
  filter(DEVICE_GEO_REGION == "xor") %>%
  mutate(
    zip_is_or = substr(DEVICE_GEO_ZIP, 1, 2) == "97",
    geo_is_or = DEVICE_GEO_LAT >= oregon_lat_min &
      DEVICE_GEO_LAT <= oregon_lat_max &
      DEVICE_GEO_LONG >= oregon_long_min &
      DEVICE_GEO_LONG <= oregon_long_max,
    either_or = zip_is_or | geo_is_or
  ) %>%
  summarise(
    n_total      = n(),
    n_zip_or     = sum(zip_is_or, na.rm = TRUE),
    n_geo_or     = sum(geo_is_or, na.rm = TRUE),
    n_either_or  = sum(either_or, na.rm = TRUE),
    n_neither    = sum(!either_or, na.rm = TRUE)
  )

xor_check
```

This result justifies that `"xor"` is also representative of Oregon.

**Observations**

  - `DEVICE_GEO_REGION` is stored as a `character` field and has no missing values.
  - There are four distinct values: "OR", "Or", "oregon", "xor"
  - "OR", "Or", and "oregon" clearly represent Oregon; "xor" is shown to represent Oregon through further analysis of Zip code and latitudinal/longitudinal values.

**Decisions:**

  - Standardize all Oregon variants (`"OR"`, `"Or"`, `"oregon"`, `"xor"`) to a single clean code, `"OR"`.
  
### Cleaning - `DEVICE_GEO_REGION`

```{r region-clean}
ad_clean <- ad_clean %>%
  mutate(
    DEVICE_GEO_REGION_clean = case_when(
      DEVICE_GEO_REGION %in% c("OR", "Or", "oregon", "xor") ~ "OR",
      TRUE ~ NA_character_
    )
  )

# Count the total number of rows affected
n_region_changed <- sum(ad_clean$DEVICE_GEO_REGION_clean != ad_clean$DEVICE_GEO_REGION,
                        na.rm = TRUE)
```

Log entry for region variable:

```{r region-log}
log_change(
  step_id       = "GEO1",
  variable      = "DEVICE_GEO_REGION",
  issue         = "Inconsistent Oregon region codes ('OR', 'Or', 'oregon', 'xor').",
  decision      = "Standardized all Oregon variants to 'OR'.",
  rows_affected = n_region_changed
)
```

### Exploration – `DEVICE_GEO_CITY`

We begin by checking for basic formatting issues:

```{r city-whitespace}
# Check for leading/trailing whitespace
city_ws <-
  sum(str_detect(ad_clean$DEVICE_GEO_CITY, "^\\s|\\s$"),
      na.rm = TRUE)

# Count distinct non-missing values
n_city_unique <- ad_clean %>%
  filter(!is.na(DEVICE_GEO_CITY)) %>%
  summarise(n = n_distinct(DEVICE_GEO_CITY)) %>%
  pull(n)

# Count new number of NA values
n_city_na <- sum(is.na(ad_clean$DEVICE_GEO_CITY))

cat("Rows with whitespace in city names:", city_ws, "\n")
cat("Distinct non-missing city strings:", n_city_unique, "\n")
cat("Rows with missing DEVICE_GEO_CITY:", n_city_na, "\n")
```

**Observations**

  - `DEVICE_GEO_CITY` is a free-text field with **many unique values** (`r n_city_unique`), including small communities and non-city place names.
  - There are no rows with and leading or trailing whitespace.
  - There are now **`r n_city_na` missing values** after removal of rows in previous steps.  
  - Since all impressions are known to originate from Oregon (via ZIP/Region/Lat–Long validation), **missing city names do not invalidate a row**.

**Decisions**

  - Preserve all city names as-is, including unincorporated localities or variant spellings.
  - Retain NA values as they still represent Oregon impressions.

### Cleaning – `DEVICE_GEO_CITY`

No transformation of `DEVICE_GEO_CITY` is required. 

Create a pass-through cleaned column for schema consistency:

```{r city-clean}
ad_clean <- ad_clean %>%
  mutate(DEVICE_GEO_CITY_clean = DEVICE_GEO_CITY)
```

Because **no values were modified**, **no log entry is added** for this variable.

### Summary

  - `DEVICE_GEO_CITY` contains a large variety of locality names, which is expected for IP-derived geodata.  
  - `r n_city_na` rows contain missing city information; All rows are otherwise confirmed as Oregon $\to$ these are retained.
  - No rows were removed based on city information.  

### Exploration - `DEVICE_GEO_ZIP`

We treat ZIP codes as **strings**.

```{r zip-explore, results='hold'}
# Basic checks
zip_class <- class(ad_clean$DEVICE_GEO_ZIP)

n_zip_na <- sum(is.na(ad_clean$DEVICE_GEO_ZIP))
n_zip_unique <- ad_clean %>%
  filter(!is.na(DEVICE_GEO_ZIP)) %>%
  summarise(n = n_distinct(DEVICE_GEO_ZIP)) %>%
  pull(n)

cat("Class of DEVICE_GEO_ZIP:      ", zip_class, "\n")
cat("Distinct non-missing ZIPs:    ", n_zip_unique, "\n")
cat("Rows with missing ZIP (now):  ", n_zip_na, "\n")

```

  - We see `DEVICE_GEO_ZIP` is the correct type, has 274 distinct ZIP values (non-NA), and now has 21,076 rows with `NA` values.
  
We now run full diagnostic checks:

```{r zip-diagnostics, results='hold'}
# Define ZIP helper structure
zip_diag_tbl <- ad_clean %>%
  mutate(
    # Trim whitespace for diagnostic purposes
    zip_trim = str_trim(DEVICE_GEO_ZIP),
    
    # Structural checks
    has_ws = str_detect(DEVICE_GEO_ZIP, "^\\s|\\s$"),
    bad_length = str_length(zip_trim) != 5,
    starts_negative = str_starts(zip_trim, "-"),
    
    # Extract candidate 5-digit ZIP (for Oregon-range check)
    zip5 = str_sub(zip_trim, 1, 5),
    
    # Check if first 5 chars are digits
    zip5_is_digits = str_detect(zip5, "^[0-9]{5}$"),
    
    # Convert only valid 5-digit strings to integer
    zip5_int = if_else(zip5_is_digits, as.integer(zip5), NA_integer_),
    
    # Oregon ZIP code validity check
    out_of_or_range = if_else(
      !is.na(zip5_int) & (zip5_int < 97001L | zip5_int > 97920L),
      TRUE,
      FALSE
    )
  )

# Summaries
zip_diag <- zip_diag_tbl %>%
  summarise(
    n_total_rows = n(),
    n_whitespace = sum(has_ws, na.rm = TRUE),
    n_bad_length = sum(bad_length, na.rm = TRUE),
    n_negative_start = sum(starts_negative, na.rm = TRUE),
    n_non_digit = sum(!zip5_is_digits, na.rm = TRUE),
    n_missing = sum(is.na(DEVICE_GEO_ZIP)),
    
    # Oregon-range issues (only applies to true digit-strings)
    n_out_of_range   = sum(out_of_or_range, na.rm = TRUE),
    n_valid_oregon   = sum(!out_of_or_range & !is.na(zip5_int)),
    n_invalid_zip5   = sum(is.na(zip5_int))   # includes non-digit, wrong length, etc.
  )

zip_diag
```

  - Of the `r zip_diag$n_total_rows` rows, **`r zip_diag$n_valid_oregon`** have structurally valid Oregon ZIPs (5 digits, in the 97001-97920 range).
  - There are **`r zip_diag$n_missing`** rows with `NA` ZIP values.
  - **`r zip_diag$n_bad_length`** rows have a length not equal to 5, **`r zip_diag$n_negative_start`** rows start with a negative sign, and **`r zip_diag$n_non_digit`** rows contain non-digit characters in the first five positions.

We next investigate which ZIP values appear in these 18 problematic rows:

```{r badrow-checks}
bad_zip_rows <- zip_diag_tbl %>%
  filter(bad_length |
           starts_negative |
           !zip5_is_digits)

bad_zip_patterns <- bad_zip_rows %>%
  distinct(zip_trim) %>%
  arrange(zip_trim)

bad_zip_patterns
```

We explore whether the `NA` values can be retrieved using Latitudinal and Longitudinal values:

```{r zip-lat-long-check}
zip_lat_long_check <- ad_clean %>%
  group_by(DEVICE_GEO_LAT, DEVICE_GEO_LONG) %>%
  summarise(
    n_zip = n_distinct(DEVICE_GEO_ZIP[!is.na(DEVICE_GEO_ZIP)]),
    .groups = "drop"
  ) %>%
  filter(n_zip > 1)

# Number of lat/long pairs mapping to multiple ZIPs
nrow(zip_lat_long_check)
```

We see that lat/long values can be associated with more than one zip code and therefore cannot be used to retrieve missing zip values.

**Observations**

  - `DEVICE_GEO_ZIP` is stored as a character vector, as expected for ZIP codes.  
  - The majority of non-missing ZIP values are structurally valid 5-digit Oregon ZIP codes between 97001 and 97920.  
  - A small number of rows (18) use a sentinel-style value (`"-999"`), which is:
  - Missing ZIPs (`NA`) are relatively common and **co-occur with missing city information**, reflecting upstream geolocation lookup failures rather than formatting errors.  

**Decisions**

  - Create `DEVICE_GEO_ZIP_clean` with sentinel values (`"-999"`) as `NA` and all other values as-is.
  - **We will not attempt to reconstruct missing ZIPs** from city or latitude/longitude due to:
    - City $\to$ ZIP is not one-to-one in Oregon
    - Reverse geocoding from lat/long would require external data and would be speculative

### Cleaning – `DEVICE_GEO_ZIP`

```{r zip-clean, results='hold'}
# Sentinel codes to treat as true missing
sentinels <- c("-999")

ad_clean <- ad_clean %>%
  mutate(
    DEVICE_GEO_ZIP_clean = case_when(
      DEVICE_GEO_ZIP %in% sentinels ~ NA_character_,
      TRUE ~ DEVICE_GEO_ZIP
    )
  )

# Count rows changed
n_zip_changed <- sum(ad_clean$DEVICE_GEO_ZIP %in% sentinels)

cat("ZIP rows changed due to sentinel removal: ", n_zip_changed, "\n")
```

Log entries for zip variable:

```{r zip-log, results='hold'}
log_change(
  step_id       = "GEO2",
  variable      = "DEVICE_GEO_ZIP",
  issue         = "Sentinel ZIP value '-999' not a valid Oregon ZIP.",
  decision      = "Converted sentinel ZIPs to NA; preserved all other ZIP values as-is.",
  rows_affected = n_zip_changed
)
```

### Summary

  - `DEVICE_GEO_ZIP` is treated as a **string field**, which is appropriate for postal codes.  
  - The vast majority of ZIP values are valid Oregon ZIP codes and are transferred as-is to `DEVICE_GEO_ZIP_clean`.  
  - A small number of sentinel or malformed values (e.g., `"-999"`) are converted to `NA`, as they cannot represent real ZIP codes. 
  - No other structural issues were present in the ZIP field.  
  - Missing values remain missing and are **not reconstructed** to avoid speculative geolocation inference.  
  - No rows were removed during these data cleaning steps.


### Exploration - `DEVICE_GEO_LAT` \&`DEVICE_GEO_LONG` 

`DEVICE_GEO_LAT` and `DEVICE_GEO_LONG` are both stored as `numeric` and contain **no missing values**.

We begin exploration by comparing observed values to Oregon’s approximate geographic bounds:
  
  - Latitude (Oregon approx.): **$42^\circ$ N to $46.15^\circ$ N**  
  - Longitude (Oregon approx.): **$-124.3^\circ$ W to $-116.45^\circ$ W**

```{r lat-long-summary, message=FALSE}
# Added slight buffers to values
latlong_summary <- ad_clean %>%
  summarise(
    n_rows          = n(),
    min_lat         = min(DEVICE_GEO_LAT, na.rm = TRUE),
    max_lat         = max(DEVICE_GEO_LAT, na.rm = TRUE),
    n_lat_outside   = sum(DEVICE_GEO_LAT < 41    |
                            DEVICE_GEO_LAT > 47,     na.rm = TRUE),
    min_long        = min(DEVICE_GEO_LONG, na.rm = TRUE),
    max_long        = max(DEVICE_GEO_LONG, na.rm = TRUE),
    n_long_outside  = sum(DEVICE_GEO_LONG < -125 |
                            DEVICE_GEO_LONG > -116, na.rm = TRUE)
  )

latlong_summary %>%
  kable(caption = "Lat/long values compared to approximate Oregon bounds") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```  

  - **Latitude:** All latitude values fall within a plausible Oregon range.
  - **Longitude:** Exactly 100 rows have **longitudes** outside Oregon’s bounds.

To explore these out-of-bound rows, we investigate their corresponding **Zip** and **City** values. We first check to see how many of the 100 have `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP` data available:

```{r bad-long-na-check}
# Rows with longitude outside Oregon bounds
bad_long <- ad_clean %>%
  filter(DEVICE_GEO_LONG < -125 | DEVICE_GEO_LONG > -116)

bad_long %>%
  summarise(
    n_bad         = n(),
    n_zip_city_ok = sum(!is.na(DEVICE_GEO_ZIP_clean) &
                          !is.na(DEVICE_GEO_CITY)),
    n_zip_na      = sum(is.na(DEVICE_GEO_ZIP_clean)),
    n_city_na     = sum(is.na(DEVICE_GEO_CITY))
  )
```

We see that **98 of the 100** rows have `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP_clean` data available. We first address those rows.

```{r zip-city-shift-check}
# "Good" longitudes: within Oregon bounds
good_long <- ad_clean %>%
  filter(DEVICE_GEO_LONG >= -125, DEVICE_GEO_LONG <= -116)

# Collapse "good" longitudes to one reference value per ZIP + CITY
good_zipcity <- good_long %>%
  filter(!is.na(DEVICE_GEO_ZIP_clean), !is.na(DEVICE_GEO_CITY)) %>%
  group_by(DEVICE_GEO_ZIP_clean, DEVICE_GEO_CITY) %>%
  summarise(ref_long = mean(DEVICE_GEO_LONG, na.rm = TRUE),
            .groups  = "drop")

# Join bad rows (w/ valid ZIP + CITY) to these reference values
paired_zipcity <- bad_long %>%
  filter(!is.na(DEVICE_GEO_ZIP_clean), !is.na(DEVICE_GEO_CITY)) %>%
  left_join(good_zipcity,
            by = c("DEVICE_GEO_ZIP_clean", "DEVICE_GEO_CITY")) %>%
  mutate(diff_long = ref_long - DEVICE_GEO_LONG)

# Summarize differences
paired_zipcity %>%
  summarise(
    n_bad_with_match = sum(!is.na(ref_long)),
    min_diff         = min(diff_long, na.rm = TRUE),
    max_diff         = max(diff_long, na.rm = TRUE),
    mean_diff        = mean(diff_long, na.rm = TRUE),
    n_near_10        = sum(abs(diff_long - 10) < 0.1, na.rm = TRUE)
  )
```

These results **show a 10 degree shift** for these 98 values.

We now address the **2 values with `NA` for `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP_clean`**. 
We compare these to other rows with the same `AUCTION_ID`:

```{r auction-shift-check-clean, message=FALSE}
# Identify bad rows with missing ZIP/CITY
bad_missing_loc <- bad_long %>%
  filter(is.na(DEVICE_GEO_ZIP_clean) | is.na(DEVICE_GEO_CITY))

# Reference longitudes for each auction (from good rows)
good_auction <- good_long %>%
  group_by(AUCTION_ID) %>%
  summarise(ref_long_auction = mean(DEVICE_GEO_LONG, na.rm = TRUE),
            .groups = "drop")

# Join & compute shift difference
auction_shift_check <- bad_missing_loc %>%
  left_join(good_auction, by = "AUCTION_ID") %>%
  mutate(diff_long = ref_long_auction - DEVICE_GEO_LONG) %>%
  select(AUCTION_ID,
         DEVICE_GEO_LAT,
         DEVICE_GEO_LONG,
         ref_long_auction,
         diff_long)

auction_shift_check %>%
  kable(caption = "Two NA-location rows with auction-based longitude comparison") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```

These results show these two rows are **shifted exactly 10 degrees** from the other longitude values of rows with the same respective `AUCTION_ID`, justifying a shift for these rows as well.

We now verify that all of the `NA` rows will be within Oregon bounds if shifted by 10:

```{r check-shift}
# Verify shift will put "bad longitude" rows in bounds
bad_long_shift_check <- bad_long %>%
  mutate(long_shifted = DEVICE_GEO_LONG + 10) %>%
  summarise(
    n_rows                = n(),
    min_long_orig         = min(DEVICE_GEO_LONG, na.rm = TRUE),
    max_long_orig         = max(DEVICE_GEO_LONG, na.rm = TRUE),
    min_long_shifted      = min(long_shifted, na.rm = TRUE),
    max_long_shifted      = max(long_shifted, na.rm = TRUE),
    n_shifted_in_bounds   = sum(long_shifted >= -125 &
                                  long_shifted <= -116, na.rm = TRUE),
    within_oregon_bounds  = (min(long_shifted, na.rm = TRUE) >= -125 & max(long_shifted, na.rm = TRUE) <= -116)
  )

bad_long_shift_check
```

We see that *all shifted values* fall within the Oregon range for longitude. 

**Observations**

  - `DEVICE_GEO_LAT`
      - All values fall within Oregon’s plausible latitude range.
      - No cleaning is necessary.
  - `DEVICE_GEO_LONG`
      - Exactly **100 rows** are out-of-range.
      - ZIP + City comparisons (98 rows), Auction-level comparison (2 rows), and range verification all confirm a consistent 10 degree systematic shift.
      
**Decisions**

  - **Latitude**
      - Leave unchanged.
      - Create `DEVICE_GEO_LAT_clean` as a direct copy.
  - **Longitude**
      - Correct the **100 systematically shifted values** by adding 10.
      - All other longitude values remain unchanged.
      - Create a cleaned field: `DEVICE_GEO_LONG_clean`

### Cleaning - `DEVICE_GEO_LONG` &`DEVICE_GEO_LONG` 

```{r longitude-clean}
ad_clean <- ad_clean %>%
  mutate(
    DEVICE_GEO_LAT_clean  = DEVICE_GEO_LAT,
    DEVICE_GEO_LONG_clean = case_when(
      DEVICE_GEO_LONG < -125 |
        DEVICE_GEO_LONG > -116 ~ DEVICE_GEO_LONG + 10,
      TRUE ~ DEVICE_GEO_LONG
    )
  )

# Count rows corrected
n_long_shifted <-
  sum(ad_clean$DEVICE_GEO_LONG_clean != ad_clean$DEVICE_GEO_LONG)
cat("Longitude values corrected (shifted by +10): ",
    n_long_shifted,
    "\n")
```

Log entry for longitudinal variable:

```{r longitude-log}
log_change(
  step_id = "GEO3",
  variable = "DEVICE_GEO_LONG",
  issue = "100 longitude values were systematically shifted outside Oregon bounds by ~10 degrees.",
  decision = "Corrected all out-of-range longitude values by applying a +10 degree shift.",
  rows_affected = n_long_shifted
)
```

### Summary

  - `DEVICE_GEO_LAT` required no changes; all values were already valid.
  - `DEVICE_GEO_LONG` contained 100 systematically shifted values, all offset by $+10^\circ$.
  - Three independent analytical checks (ZIP + City matching, auction-level comparison, and range verification) confirmed the correction.
  - A cleaned longitude variable (`DEVICE_GEO_LONG_clean`) now restores all observations to plausible Oregon coordinates.
  - No rows were removed; only 100 longitude values were corrected. 
   

---

## **AD SIZES:** `REQUESTED_SIZES`, `SIZE`

---

## **PRICING:** `PRICE`

---

## **LATENCY METRIC:** `RESPONSE_TIME`

---

## **OUTCOME:** `BID_WON`


---

---


# **VARIABLE**

### Description

- `VARIABLE`  
  - **Type (dictionary):** `TYPE`  
  - **Meaning:** 
  - **Expectation:** 
  - **Actual Type:** `character`
  - **Missing:** None

- `VARIABLE`  
  - **Type:** `TYPE`  
  - **Meaning:** 
  - **Expectation:** 
  - **Actual:** `character`
  - **Missing:** None
  
### Exploration


**Observations**

  - 
  - 
  - 
  - 

**Decisions:**

  - 
  - 

### Cleaning
Log entries for time variables:

```{r example-log}
log_change(
  step_id = "T1",
  variable = "TIMESTAMP",
  issue = "Inconsistent TIMESTAMP formats, including 'NA' date tokens.",
  decision = "Recover 'NA' dates using `DATE_UTC`; parsed to POSIXct (UTC).",
  rows_affected = nrow(ad_clean)
)

log_change(
  step_id = "T2",
  variable = "DATE_UTC",
  issue = "DATE_UTC stored as character.",
  decision = "Converted to Date type.",
  rows_affected = nrow(ad_clean)
)
```

### Summary

  - 
  - 
   
   
   
   
   
   
   
   
---

---

# **CLEANED DATASET**

## Assemble `ad_clean`

## # Post-Cleaning Inspection

  - Final sanity checks on NA structure, bounds, duplicates

## Save via `data_io.R` function `export_ad_data()`

---

---

# **DATA LOG**

The table below records all data-cleaning actions taken throughout this notebook.  
Each entry corresponds to a specific issue that was identified and the decision made to address it.

## Log

```{r data-log-display}
cleaning_log %>%
  dplyr::arrange(step_id) %>%
  knitr::kable(
    caption = "Summary of Data-Cleaning Decisions",
    align   = "l"
  ) %>%
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```

## Log Key

The prefixes below are used in the `step_id` field of the data-cleaning log.  
Each prefix corresponds to a logical group of variables in the dataset.

| Prefix | Group            | Variables Included                         |
|--------|------------------|--------------------------------------------|
| G      | Global           | Entire Dataset                             |
| T      | Time             | TIMESTAMP, DATE_UTC                        |
| ID     | Identifiers      | AUCTION_ID, PUBLISHER_ID                   |
| DT     | Device           | DEVICE_TYPE                                |
| GEO    | Geolocation      | DEVICE_GEO_{REGION, CITY, ZIP,  LAT, LONG} |
| SZ     | Ad Sizes         | SIZE, REQUESTED_SIZES                      |
| PR     | Pricing          | PRICE                                      |
| RT     | Latency          | RESPONSE_TIME                              |
| BW     | Outcome          | BID_WON                                    |