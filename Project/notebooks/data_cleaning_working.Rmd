---
title: "`Project/notebooks/data_cleaning.R`"
author: "TECK Squad"
date: "December 12, 2025"
output:
  rmdformats::readthedown:
    toc_depth: 3        
    code_folding: hide
fontsize: 11pt
geometry: margin=1in
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
# ---- Setup ----
suppressPackageStartupMessages({
  library(tidyverse)
  library(arrow)
  library(dplyr)
  library(stringr)
  #library(rprojroot)
  library(knitr)
  library(kableExtra)
  
  #library(broom)
  #library(car)
  #library(latex2exp)
  #library(MPV)
  #library(glmnet)
})

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 6.5,
  fig.height = 4
)
theme_set(theme_bw(base_size = 12))

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# ---- Helper functions ----
fmt   <- function(x, d = 3)
  round(x, d)
fmt_e <-
  function(x, digits = 3)
    formatC(x, format = "e", digits = digits)

# ---- Load data I/O utilities and raw data ----
source("Project/scripts/data_io.R")
# Raw data from parquet via `data_io.R`
ad_raw <- load_ad_data(preview = FALSE)

# ---- Data Log setup ----
cleaning_log <- tibble::tibble(
  step_id = character(),
  variable = character(),
  issue = character(),
  decision = character(),
  rows_affected = integer()
)

log_change <- function(step_id, variable, issue, decision, rows_affected = NA_integer_) {
  cleaning_log <<- cleaning_log %>%
    add_row(
      step_id       = step_id,
      variable      = variable,
      issue         = issue,
      decision      = decision,
      rows_affected = rows_affected
    )
}
```

---

# **Purpose**

This notebook documents the full data-cleaning workflow for the Oregon ad bidding dataset.

  - **Input:** raw parquet data loaded via `load_ad_data()` (see `Project/scripts/data_io.R`).
  - **Output:** a cleaned analysis-ready dataset saved under `Project/data/...`.

---

---

# **DATA OVERVIEW**

- According to the data dictionary, each row represents **one bidder’s response for one requested size within a single ad auction**, meaning:
  - Each `AUCTION_ID` appears multiple times  
  - There may be several responses for the same ad size within the same auction  

---

## Dimensions and Structure

We begin with a high-level look at the raw dataset.

```{r overview-dims, message=FALSE, 'hold'=TRUE}
# Dimensions
n_rows <- nrow(ad_raw)
n_cols <- ncol(ad_raw)

dim_table <- tibble(
  Metric = c("Number of Rows", "Number of Columns"),
             Value = c(n_rows, n_cols)
)

dim_table %>%
  kable(caption = "Dataset Dimentions") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

---

## Basic summaries

We summarize all variables numerically and inspect missingness patterns using the helper summary function defined in `data_io.R`.

```{r overview-summary}
summarize_ad_data(ad_raw)
```

The only variables with missing data are `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP`, with **21,198 missing rows** each.

---

## Expected vs. Actual Data Types

We can compare expected data types as given by the data dictionary to the actual type of each variable in the data:

```{r overview-expected-vs-actual, message=FALSE}
expected_types <- tibble(
  Variable = c(
    "TIMESTAMP",
    "DATE_UTC",
    "AUCTION_ID",
    "PUBLISHER_ID",
    "DEVICE_TYPE",
    "DEVICE_GEO_COUNTRY",
    "DEVICE_GEO_REGION",
    "DEVICE_GEO_CITY",
    "DEVICE_GEO_ZIP",
    "DEVICE_GEO_LAT",
    "DEVICE_GEO_LONG",
    "REQUESTED_SIZES",
    "SIZE",
    "PRICE",
    "RESPONSE_TIME",
    "BID_WON"
  ), 
  Expected_Class = c(
    "datetime",          # TIMESTAMP_NTZ
    "date",              # DATE
    "categorical",       # Auction ID
    "categorical",       # Publisher ID
    "categorical",       # desktop/mobile/tablet
    "categorical",       # 2-letter country
    "categorical",       # 2-letter USPS region
    "categorical",       # City name
    "categorical",       # ZIP / ZIP+4 string
    "numeric",           # Latitude
    "numeric",           # Longitude
    "categorical",       # List of sizes
    "categorical",       # Single size
    "numeric",           # CPM bid
    "numeric",           # Response time in ms
    "boolean"            # TRUE/FALSE winner
  ),
  Expected_Detail = c(
    "TIMESTAMP_NTZ (UTC event time)",
    "DATE (UTC calendar date)",
    "VARCHAR (auction identifier)",
    "VARCHAR (publisher/site identifier)",
    "VARCHAR, (device category, string)",
    "VARCHAR(2) (ISO 2-letter country code)",
    "VARCHAR(2) (USPS state/region postal code)",
    "VARCHAR, (city name, string)",
    "VARCHAR(10) (ZIP or ZIP+4)",
    "FLOAT (latitude of device)",
    "FLOAT (longitude of device)",
    "VARCHAR/ARRAY (all requested sizes, ex: '300x250, 320x50')",
    "VARCHAR (size for this bid, ex: `300x250')",
    "NUMBER(12,6) (CPM bid in USD)",
    "NUMBER(10,0) (bidder latency in ms)",
    "BOOLEAN (if bid won, TRUE/FALSE)"
  )
)

expected_types <- expected_types %>%
  arrange(factor(
    Expected_Class,
    levels = c("numeric", "categorical", "datetime", "date", "boolean")
  ))

actual_types <- tibble(
  Variable = names(ad_raw),
  Actual_Type = sapply(ad_raw, class)
)

comparison <- expected_types %>%
  left_join(actual_types, by = "Variable")

comparison %>%
  kable(caption = "Expected vs Actual Types (Data Dictionary vs Raw Data)") %>%
    kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

Many of these are suspicious or inconsistent with the data dictionary:
  
  - **`PRICE`** and **`RESPONSE_TIME`** 
      - Expected *numeric* $\to$ Stored as `character`
      - Likely due to *non-numeric characters* or inconsistent formatting.
  - **`DEVICE_TYPE`**
      - Expected descriptive *string* $\to$ Stored as an `integer` code
      - Implies an encoded representation without a visible lookup table.
  - **`DEVICE_GEO_COUNTRY`** 
      - Expected per dictionary $\to$ Not present in this data subset
  - **`TIMESTAMP`** and **`DATE_UTC`** 
      - Expected *datetime/date* $\to$ Stored as `character`
      - Will need to be converted prior to any time-based analysis.
  - **`BID_WON`** 
      - Expected *boolean* $to$ Stored as `character`
      - Will require re-coding to `logical` (TRUE/FALSE) type.

In addition, some geographic and categorical fields (**`DEVICE_GEO_REGION`**, **`DEVICE_GEO_CITY`**, and **`DEVICE_GEO_ZIP`**) appear broadly consistent with expected classes, but will need to be fully analysed for any messy real-world values such as **inconsistent casing**, **invalid state codes**, or **irregular ZIP formats**. These issues will be fully examined and addressed in the variable-level cleaning sections that follow.

---

---

# **FULL DATASET**

This section examines the dataset at the row level to ensure there are no duplicated records and to understand whether the available columns can uniquely identify each row. According to the data dictionary, the true grain of the full dataset is:

> **(AUCTION_ID, bidder/seat, SIZE, \dots)**

However, bidder- and seat-level identifiers are **not included** in this subset, so we do not expect any single column or simple composite key to uniquely identify each record.

---

## Duplicated rows

We test for full-row duplicates.

```{r full-duplicates, results='hold'}
# Count total rows and distinct rows across all columns
n_raw      <- nrow(ad_raw)
n_distinct <- ad_raw %>% distinct() %>% nrow()
n_dups     <- n_raw - n_distinct

cat("Total rows:                       ", n_raw, "\n")
cat("Distinct rows (all columns):      ", n_distinct, "\n")
cat("Exact duplicate rows (by count):  ", n_dups, "\n")
```

**Decision:** Drop exact duplicates, while keeping one copy of each unique row.

We implement our decision for handling full-row duplicates and add to data log:

```{r full-dup-clean, results='hold'}
# Remove exact full-row duplicates (keep one copy)
ad_clean <- ad_raw %>%  distinct()

log_change(
    step_id = "G1",
    variable = "ALL",
    issue = "Exact duplicate rows present",
    decision = "Dropped full duplicate rows keeping one copy.",
    rows_affected = n_dups
  )
```

---

## Keys

Since bidder/seat identifiers are missing, no single column is expected to uniquely identify a record.
We begin by testing the most intuitive composite key:

> (`AUCTION_ID`, `SIZE`, `PRICE`)

```{r key-check-intial}
simple_unique <- ad_clean %>%
  distinct(AUCTION_ID, SIZE, PRICE) %>%
  nrow()

cat("Unique composite (AUCTION_ID, SIZE, PRICE): ", simple_unique)
```

The dataset contains `r nrow(ad_clean)` rows after duplicate removal,  
but only `r simple_unique` unique combinations of (`AUCTION_ID`, `SIZE`, `PRICE`).  
Thus, this composite key does **not** uniquely identify rows.

We create a helper function to allow us to search for a combination which will work as a key. 

**NOTE:** To avoid computational blow-up, we focus only on the most likely variables and limit our number of variables per composite key to 4:

```{r composite-key-helper}
check_candidate_keys <- function(data, cols, max_size = 4, show_full = TRUE) {
  results <- list()
  idx <- 1L
  
  for (k in seq_len(max_size)) {
    combos <- combn(cols, k, simplify = FALSE)
    
    for (combo in combos) {
      n_unique <- data %>%
        distinct(across(all_of(combo))) %>%
        nrow()
      
      results[[idx]] <- tibble(
        combo   = paste(combo, collapse = " + "),
        size    = k,
        n_rows  = nrow(data),
        n_unique = n_unique,
        is_key  = (n_unique == nrow(data))
      )
      idx <- idx + 1L
    }
  }
  
  results <- bind_rows(results)
  
  if (show_full) {
    # Full table (shown if needed for further exploration)
    results %>%
      arrange(desc(is_key), size, desc(n_unique))
  } else {
    # Compact summary
    results %>%
      group_by(size) %>%
      summarise(
        num_combos   = n(),
        num_keys     = sum(is_key),
        max_n_unique = max(n_unique),
        .groups = "drop"
      ) %>%
      arrange(size)
  }
}
```

We then use this function to check the most likely candidate variables:

```{r composite-key-search}
composite_candidates <- c(
  "AUCTION_ID",
  "SIZE",
  "PRICE",
  "RESPONSE_TIME",
  "BID_WON",
  "PUBLISHER_ID",
  "DEVICE_TYPE",
  "DEVICE_GEO_CITY",
  "DEVICE_GEO_ZIP",
  "DEVICE_GEO_LAT",
  "DEVICE_GEO_LONG",
  "TIMESTAMP"
)

key_results <- check_candidate_keys(
  data     = ad_clean,
  cols     = composite_candidates,
  max_size = 4,
  show_full = FALSE
)

key_results
```

From these results, we can see that no combination (up to four variables) of these candidate variables will create a composite key to uniquely identify the rows of this dataset.

---

## Summary

  - Exact duplicate rows were removed (`r n_dups` rows).
  - No reasonable variable or combination of variables uniquely identifies each row.  
  This is consistent with the data dictionary, which indicates that the true grain requires `bidder/seat`-level fields not present in this subset.

We proceed using **`ad_clean`** as our working dataset for variable-level cleaning.

---

---

# **VARIABLE-LEVEL CLEANING**

The following sections clean each variable (or group of related variables) in an order that follows the data dictionary.  
For each variable, we document:

  - The expected type and meaning  
  - Exploration of raw values  
  - Cleaning decisions  
  - Updates to the data-cleaning log  
  - A post-clean summary  

---

## **TIME VARIABLES:** `TIMESTAMP` \& `DATE_UTC`

### Description

- `TIMESTAMP`  
  - **Type:** `TIMESTAMP_NTZ`  
  - **Meaning:** Event time (UTC) when the log row was recorded 
  - **Expectation:** A full datetime (date + time), treated as UTC
  - **Actual:** `character`
  - **Missing:** None

- `DATE_UTC`  
  - **Type:** `DATE`  
  - **Meaning:** Calendar date (UTC) used for partitioning / grouping
  - **Expectation:** A date consistent with the date portion of `TIMESTAMP`
  - **Actual:** `character`
  - **Missing:** None

### Exploration

Sample a few `TIMESTAMP` values:
```{r timestamp-sample}
set.seed(42)
sample(ad_clean$TIMESTAMP, size = 20)
```


```{r timestamp-count-format, results='hold'}
dash_rows       <- str_detect(ad_clean$TIMESTAMP, "-") %>%  sum()
slash_rows      <- str_detect(ad_clean$TIMESTAMP, "/") %>%  sum()
na_date_rows    <- str_detect(ad_clean$TIMESTAMP, "^NA") %>%  sum()

cat("Rows with '-' in date portion:   ", dash_rows,  "\n")
cat("Rows with '/' in date portion:   ", slash_rows, "\n")
cat("Rows starting with 'NA ' date:   ", na_date_rows, "\n")
cat("These three account for all rows:", 
    dash_rows + slash_rows + na_date_rows == nrow(ad_clean))
```

**Observations**

  - `r dash_rows` of `TIMESTAMP` values use a dash-separated date (e.g., `"2025-10-12 14:03:27"`).
  - `r slash_rows` uses a slash-separated format (e.g., `"10/12/2025 14:03:27"`).  
  - `r na_date_rows` appear with an `NA` date and a valid time-of-day (e.g., `"NA 14:03:27"`).  
  - The raw columns themselves do **not** contain missing values: all rows have some string in `TIMESTAMP` and `DATE_UTC`. 

**Decisions:**

  - Recover missing dates in `TIMESTAMP` via `DATE_UTC`
  - Parse `TIMESTAMP` into a POSIXct (UTC) variable with multiple formats.
  - Convert `DATE_UTC` to *Date* type, keeping raw string as-is.
  
### Cleaning

Recover `TIMESTAMP` where the date part was missing but time-of-day exists, using `DATE_UTC` as the date source:

```{r timestamp-recover-date}
# Extract date and time components
ad_clean <- ad_clean %>%
  mutate(
    ts_date_raw = str_extract(TIMESTAMP, "^[^ ]+"),
    ts_time_raw = str_extract(TIMESTAMP, "\\d{1,2}:\\d{1,2}:\\d{1,2}")
  )

# Fix rows where the date is 'NA', keeping all others as-is
ad_clean <- ad_clean %>%
  mutate(TIMESTAMP_fixed = case_when(
    ts_date_raw == "NA" & !is.na(DATE_UTC) & !is.na(ts_time_raw) ~
      paste(DATE_UTC, ts_time_raw),
    TRUE ~ TIMESTAMP
  ))
```

Parse the fixed `TIMESTAMP` into POSIXct:

```{r timestamp-parse}
ad_clean <- ad_clean %>%
  mutate(TIMESTAMP_clean = lubridate::parse_date_time(
    TIMESTAMP_fixed,
    orders = c("ymd HMS", "mdy HMS"),
    tz = "UTC"
  ))

n_na_ts <- sum(is.na(ad_clean$TIMESTAMP_clean))
cat("Rows with NA in TIMESTAMP_clean after parsing: ", n_na_ts, "\n")
```

Convert `DATE_UTC` to Date:

```{r date_utc-to-date}
ad_clean <- ad_clean %>%
  mutate(DATE_UTC_clean = as.Date(DATE_UTC))

cat("Rows with NA in DATE_UTC_clean: ", sum(is.na(ad_clean$DATE_UTC_clean)), "\n")
```

Log entries for time variables:

```{r time-log}
log_change(
  step_id = "T1",
  variable = "TIMESTAMP",
  issue = "Inconsistent TIMESTAMP formats, including 'NA' date tokens.",
  decision = "Recover 'NA' dates using `DATE_UTC`; parsed to POSIXct (UTC).",
  rows_affected = nrow(ad_clean)
)

log_change(
  step_id = "T2",
  variable = "DATE_UTC",
  issue = "DATE_UTC stored as character.",
  decision = "Converted to Date type.",
  rows_affected = nrow(ad_clean)
)
```

### Summary

  - `TIMESTAMP`: 
      - Inconsistently formatted (mixture of dash- vs. slash-separated dates and a small number of entries with an `NA` date token).
      - We repaired rows with an `NA` date by using the date from `DATE_UTC`, then parsed the corrected string into `TIMESTAMP_clean` (POSIXct, UTC).  

  - `DATE_UTC`:
      - Converted from character to `DATE_UTC_clean` (Date type).  
      
  - No rows were dropped as part of Time Variables cleaning steps.

---

## **IDENTIFIERS:** `AUCTION_ID` \& `PUBLISHER_ID`

### Description

- `AUCTION_ID`  
  - **Type (dictionary):** `VARCHAR`  
  - **Meaning:** Unique identifier for a single auction.  
  - **Expectation:** Repeated across multiple rows (one auction $to$ many bid lines).  
  - **Actual:** `character`  
  - **Missing:** None  

- `PUBLISHER_ID`  
  - **Type (dictionary):** `VARCHAR`  
  - **Meaning:** Identifier for the publisher / site / app running the auction.  
  - **Expectation:** Medium/high cardinality; used for grouping and filtering.  
  - **Actual:** `character`  
  - **Missing:** None  

### Exploration

These are opaque identifiers:

  - We do **not** attempt to interpret or recode their values
  - We only ensure they are well-formed and structurally consistent with the data dictionary.

We begin with basic uniqueness and format checks.

```{r ids-basic-explore, results='hold'}
ad_clean %>%
  summarise(
    n_rows                 = n(),
    n_auctions             = n_distinct(AUCTION_ID),
    n_publishers           = n_distinct(PUBLISHER_ID),
    avg_rows_per_auction   = n_rows / n_auctions,
    avg_rows_per_publisher = n_rows / n_publishers
  )
```

Check for leading/trailing whitespace:

```{r ids-whitespace-missing, results='hold'}
auct_ws <- sum(str_detect(ad_clean$AUCTION_ID, "^\\s|\\s$"))
pub_ws  <- sum(str_detect(ad_clean$PUBLISHER_ID, "^\\s|\\s$"))

cat("AUCTION_ID rows with whitespace:  ", auct_ws, "\n")
cat("PUBLISHER_ID rows with whitespace:", pub_ws,  "\n")
```

  - All identifier fields appear to be free of whitespace issues.

The client clarified that an auction is expected to have **exactly one** winning bid. 
We evaluate this assumption:

```{r ids-auction-outcome, results='hold'}
auction_win_summary <- ad_clean %>%
  group_by(AUCTION_ID) %>%
  summarise(
    n_rows = n(),
    n_won  = sum(BID_WON == "TRUE",  na.rm = TRUE),
    n_lost = sum(BID_WON == "FALSE", na.rm = TRUE),
    .groups = "drop"
  )

# Auctions violating the "exactly one winner" rule
auctions_multiwin <- auction_win_summary %>% filter(n_won > 1)
auctions_zerowin  <- auction_win_summary %>% filter(n_won == 0)

cat("Auctions with >1 winning bid:  ", nrow(auctions_multiwin), "\n")
cat("Auctions with 0 winning bids:  ", nrow(auctions_zerowin),  "\n")
```

We investigate how many rows are affected by these problematic auctions:

```{r problematic-auction-rows, results='hold'}
multiwin_ids <- auctions_multiwin$AUCTION_ID
zerowin_ids  <- auctions_zerowin$AUCTION_ID

multiwin_rows <- ad_clean %>%
  filter(AUCTION_ID %in% multiwin_ids) %>%
  nrow()

zerowin_rows <- ad_clean %>%
  filter(AUCTION_ID %in% zerowin_ids) %>%
  nrow()

cat("Rows in multi-winner auctions: ", multiwin_rows, "\n")
cat("Rows in zero-winner auctions:  ", zerowin_rows,  "\n")
cat("Total problematic rows:        ", multiwin_rows + zerowin_rows, "\n")
```

**Observations**

  - Multi-winner auctions: **`r nrow(auctions_multiwin)` auctions**, **`r multiwin_rows` rows**  
  - Zero-winner auctions: **`r nrow(auctions_zerowin)` auctions**, **`r zerowin_rows` rows**  
  - Combined problematic rows represent  **`r round((multiwin_rows + zerowin_rows) / nrow(ad_clean) * 100, 4)` \%**  
    of the dataset.
  - These cases are **logically impossible** under the client’s business rules and indicate corrupted or incomplete auction records.

**Decision:**  
Since an auction must have *exactly one* winner, correcting the winner is impossible, and the number of rows affected is a very small fraction of the dataset, we remove all auctions where `n_won != 1`.

### Cleaning

Identifier fields (`AUCTION_ID`, `PUBLISHER_ID`) require **minimal cleaning**:

  - No missing values
  - No leading/trailing whitespace
  - Behavior matches expectations

We still create trimmed versions for consistency:

```{r ids-clean, results='hold'}
ad_clean <- ad_clean %>%
  mutate(
    AUCTION_ID_clean   = str_trim(as.character(AUCTION_ID)),
    PUBLISHER_ID_clean = str_trim(as.character(PUBLISHER_ID))
  )
```

We then remove rows belonging to zero-win or multi-win auctions:

```{r ids-remove-invalid-auctions, results='hold'}
# Auctions that violate business logic: n_won != 1
invalid_ids <- auction_win_summary %>%
  filter(n_won != 1) %>%
  pull(AUCTION_ID)

n_invalid_rows <- ad_clean %>%
  filter(AUCTION_ID %in% invalid_ids) %>%
  nrow()

# Remove invalid auctions
ad_clean <- ad_clean %>%
  filter(!AUCTION_ID %in% invalid_ids)

cat("Total rows removed due to invalid auction outcome structure: ",
    n_invalid_rows, "\n")
```

Log entries for identifier variables:

```{r ids-log, results='hold'}
log_change(
  step_id = "I1",
  variable = "AUCTION_ID",
  issue = "Auctions with zero or multiple winners violate business rules.",
  decision = "Removed all rows belonging to auctions where n_won != 1.",
  rows_affected = n_invalid_rows
)
```

### Summary

  - Both identifier fields were already well-formed:
      - No missing values  
      - No whitespace issues  
      - High-cardinality and consistent structure  
  - Clean versions (`AUCTION_ID_clean`, `PUBLISHER_ID_clean`) were created for consistency.
  - Structural validation revealed **invalid auctions** with zero or multiple winners.  
      - These represent impossible events under domain rules, and correcting them is not feasible.  
      - A total of **`r n_invalid_rows`** rows were removed to ensure dataset consistency.  
  - The remaining dataset now contains only auctions with a valid outcome structure.

---

## **DEVICE ATTRIBUTE:** `DEVICE_TYPE`

---

## **GEOLOCATION FIELDS:** `DEVICE_GEO_REGION`, `DEVICE_GEO_CITY` \& `DEVICE_GEO_ZIP`
## **GEOLOCATION FIELDS:** `DEVICE_GEO_LAT` \& `DEVICE_GEO_LONG`

---

## **AD SIZES:** `REQUESTED_SIZES` \& `SIZE`

---

## **PRICING:** `PRICE`

---

## **LATENCY METRIC:** `RESPONSE_TIME`

---

## **OUTCOME:** `BID_WON`


---

---


# **VARIABLE**

### Description

- `VARIABLE`  
  - **Type:** `TYPE`  
  - **Meaning:** 
  - **Expectation:** 
  - **Actual:** `character`
  - **Missing:** None

- `VARIABLE`  
  - **Type:** `TYPE`  
  - **Meaning:** 
  - **Expectation:** 
  - **Actual:** `character`
  - **Missing:** None
  
### Exploration


**Observations**

  - 
  - 
  - 
  - 

**Decisions:**

  - 
  - 

### Cleaning
Log entries for time variables:

```{r example-log}
log_change(
  step_id = "T1",
  variable = "TIMESTAMP",
  issue = "Inconsistent TIMESTAMP formats, including 'NA' date tokens.",
  decision = "Recover 'NA' dates using `DATE_UTC`; parsed to POSIXct (UTC).",
  rows_affected = nrow(ad_clean)
)

log_change(
  step_id = "T2",
  variable = "DATE_UTC",
  issue = "DATE_UTC stored as character.",
  decision = "Converted to Date type.",
  rows_affected = nrow(ad_clean)
)
```

### Summary

  - `TIMESTAMP`: 
      - Inconsistently formatted (mixture of dash- vs. slash-separated dates and a small number of entries with an `NA` date token).
      - We repaired rows with an `NA` date by using the date from `DATE_UTC`, then parsed the corrected string into `TIMESTAMP_clean` (POSIXct, UTC).  

  - `DATE_UTC`:
      - Converted from character to `DATE_UTC_clean` (Date type).  
      
  - No rows were dropped as part of Time Variables cleaning steps.
   
   
   
   
   
   
   
   
---

---

# **CLEANED DATASET**

## Assemble `ad_clean`

## # Post-Cleaning Inspection

  - Final sanity checks on NA structure, bounds, duplicates

## Save via `data_io.R` function `export_ad_data()`

---

---

# **DATA LOG**

The table below records all data-cleaning actions taken throughout this notebook.  
Each entry corresponds to a specific issue that was identified and the decision made to address it.

## Log

```{r data-log-display}
cleaning_log %>%
  dplyr::arrange(step_id) %>%
  knitr::kable(
    caption = "Summary of Data-Cleaning Decisions",
    align   = "l"
  ) %>%
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```

## Log Key

The prefixes below are used in the `step_id` field of the data-cleaning log.  
Each prefix corresponds to a logical group of variables in the dataset.

| Prefix | Group            | Variables Included                         |
|--------|------------------|--------------------------------------------|
| G      | Global           | Entire Dataset                             |
| T      | Time             | TIMESTAMP, DATE_UTC                        |
| ID     | Identifiers      | AUCTION_ID, PUBLISHER_ID                   |
| DT     | Device           | DEVICE_TYPE                                |
| GEO    | Geolocation      | DEVICE_GEO_{REGION, CITY, ZIP,  LAT, LONG} |
| SZ     | Ad Sizes         | SIZE, REQUESTED_SIZES                      |
| PR     | Pricing          | PRICE                                      |
| RT     | Latency          | RESPONSE_TIME                              |
| BW     | Outcome          | BID_WON                                    |