---
title: "`Project/notebooks/data_cleaning.R`"
author: "TECK Squad"
date: "December 12, 2025"
output:
  rmdformats::readthedown:
    toc_depth: 2        
    code_folding: hide
fontsize: 11pt
geometry: margin=1in
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
# ---- Setup ----
suppressPackageStartupMessages({
  library(tidyverse)
  library(arrow)
  library(dplyr)
  #library(rprojroot)
  library(knitr)
  library(kableExtra)
  
  #library(broom)
  #library(car)
  #library(latex2exp)
  #library(MPV)
  #library(glmnet)
})

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 6.5,
  fig.height = 4
)
theme_set(theme_bw(base_size = 12))

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# ---- Helper functions ----
fmt   <- function(x, d = 3)
  round(x, d)
fmt_e <-
  function(x, digits = 3)
    formatC(x, format = "e", digits = digits)

# ---- Load data I/O utilities and raw data ----
source("Project/scripts/data_io.R")
# Raw data from parquet via `data_io.R`
ad_raw <- load_ad_data(preview = FALSE)

# ---- Data Log setup ----
cleaning_log <- tibble::tibble(
  step_id = character(),
  variable = character(),
  issue = character(),
  decision = character(),
  rows_affected = integer()
)
```

---

# **Purpose**

This notebook documents the full data-cleaning workflow for the Oregon ad bidding dataset.

  - **Input:** raw parquet data loaded via `load_ad_data()` (see `Project/scripts/data_io.R`).
  - **Output:** a cleaned analysis-ready dataset saved under `Project/data/...`.

---

---

# **DATA OVERVIEW**

- According to the data dictionary, each row represents **one bidderâ€™s response for one requested size within a single ad auction**, meaning:
  - Each `AUCTION_ID` appears multiple times  
  - There may be several responses for the same ad size within the same auction  

---

## Dimensions and Structure

We begin with a high-level look at the raw dataset.

```{r overview-dims, message=FALSE, 'hold'=TRUE}
# Dimensions
n_rows <- nrow(ad_raw)
n_cols <- ncol(ad_raw)

dim_table <- tibble(
  Metric = c("Number of Rows", "Number of Columns"),
             Value = c(n_rows, n_cols)
)

dim_table %>%
  kable(caption = "Dataset Dimentions") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

---

## Basic summaries

We summarize all variables numerically and inspect missingness patterns using the helper summary function defined in `data_io.R`.

```{r overview-summary}
summarize_ad_data(ad_raw)
```

The only variables with missing data are `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP`, with **21,198 missing rows** each.

---

## Expected vs. Actual Data Types

We can compare expected data types as given by the data dictionary to the actual type of each variable in the data:

```{r overview-expected-vs-actual, message=FALSE}
expected_types <- tibble(
  Variable = c(
    "TIMESTAMP",
    "DATE_UTC",
    "AUCTION_ID",
    "PUBLISHER_ID",
    "DEVICE_TYPE",
    "DEVICE_GEO_COUNTRY",
    "DEVICE_GEO_REGION",
    "DEVICE_GEO_CITY",
    "DEVICE_GEO_ZIP",
    "DEVICE_GEO_LAT",
    "DEVICE_GEO_LONG",
    "REQUESTED_SIZES",
    "SIZE",
    "PRICE",
    "RESPONSE_TIME",
    "BID_WON"
  ), 
  Expected_Class = c(
    "datetime",          # TIMESTAMP_NTZ
    "date",              # DATE
    "categorical",       # Auction ID
    "categorical",       # Publisher ID
    "categorical",       # desktop/mobile/tablet
    "categorical",       # 2-letter country
    "categorical",       # 2-letter USPS region
    "categorical",       # City name
    "categorical",       # ZIP / ZIP+4 string
    "numeric",           # Latitude
    "numeric",           # Longitude
    "categorical",       # List of sizes
    "categorical",       # Single size
    "numeric",           # CPM bid
    "numeric",           # Response time in ms
    "boolean"            # TRUE/FALSE winner
  ),
  Expected_Detail = c(
    "TIMESTAMP_NTZ (UTC event time)",
    "DATE (UTC calendar date)",
    "VARCHAR (auction identifier)",
    "VARCHAR (publisher/site identifier)",
    "VARCHAR, (device category, string)",
    "VARCHAR(2) (ISO 2-letter country code)",
    "VARCHAR(2) (USPS state/region postal code)",
    "VARCHAR, (city name, string)",
    "VARCHAR(10) (ZIP or ZIP+4)",
    "FLOAT (latitude of device)",
    "FLOAT (longitude of device)",
    "VARCHAR/ARRAY (all requested sizes, ex: '300x250, 320x50')",
    "VARCHAR (size for this bid, ex: `300x250')",
    "NUMBER(12,6) (CPM bid in USD)",
    "NUMBER(10,0) (bidder latency in ms)",
    "BOOLEAN (if bid won, TRUE/FALSE)"
  )
)

expected_types <- expected_types %>%
  arrange(factor(
    Expected_Class,
    levels = c("numeric", "categorical", "datetime", "date", "boolean")
  ))

actual_types <- tibble(
  Variable = names(ad_raw),
  Actual_Type = sapply(ad_raw, class)
)

comparison <- expected_types %>%
  left_join(actual_types, by = "Variable")

comparison %>%
  kable(caption = "Expected vs Actual Types (Data Dictionary vs Raw Data)") %>%
    kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

Many of these are suspicious or inconsistent with the data dictionary:
  
  - **`PRICE`** and **`RESPONSE_TIME`** 
      - Expected *numeric* $\to$ Stored as `character`
      - Likely due to *non-numeric characters* or inconsistent formatting.
  - **`DEVICE_TYPE`**
      - Expected descriptive *string* $\to$ Stored as an `integer` code
      - Implies an encoded representation without a visible lookup table.
  - **`DEVICE_GEO_COUNTRY`** 
      - Expected per dictionary $\to$ Not present in this data subset
  - **`TIMESTAMP`** and **`DATE_UTC`** 
      - Expected *datetime/date* $\to$ Stored as `character`
      - Will need to be converted prior to any time-based analysis.
  - **`BID_WON`** 
      - Expected *boolean* $to$ Stored as `character`
      - Will require re-coding to `logical` (TRUE/FALSE) type.

In addition, some geographic and categorical fields (**`DEVICE_GEO_REGION`**, **`DEVICE_GEO_CITY`**, and **`DEVICE_GEO_ZIP`**) appear broadly consistent with expected classes, but will need to be fully analysed for any messy real-world values such as **inconsistent casing**, **invalid state codes**, or **irregular ZIP formats**. These issues will be fully examined and addressed in the variable-level cleaning sections that follow.

---

---

# **FULL DATASET**

This section examines the dataset at the row level to ensure there are no duplicated records and to understand whether the available columns can uniquely identify each row. According to the data dictionary, the true grain of the full dataset is:

> **(AUCTION_ID, bidder/seat, SIZE, \dots)**

However, bidder- and seat-level identifiers are **not included** in this subset, so we do not expect any single column or simple composite key to uniquely identify each record.

---

## Duplicated rows

We test for full-row duplicates.

```{r full-duplicates, results='hold'}
# Count total rows and distinct rows across all columns
n_raw      <- nrow(ad_raw)
n_distinct <- ad_raw %>% distinct() %>% nrow()
n_dups     <- n_raw - n_distinct

cat("Total rows:                       ", n_raw, "\n")
cat("Distinct rows (all columns):      ", n_distinct, "\n")
cat("Exact duplicate rows (by count):  ", n_dups, "\n")
```

**Decision:** Drop exact duplicates, while keeping one copy of each unique row.

We implement our decision for handling full-row duplicates and add to data log:

```{r full-dup-clean, results='hold'}
# Remove exact full-row duplicates (keep one copy)
ad_clean <- ad_raw %>%  distinct()

cleaning_log <- cleaning_log %>%
  add_row(
    step_id = "G1",
    variable = "ALL",
    issue = "Exact duplicate rows present",
    decision = "Dropped full duplicate rows using distinct()",
    rows_affected = n_dups
  )
```

---

## Keys

Since bidder/seat identifiers are missing, no single column is expected to uniquely identify a record.
We begin by testing the most intuitive composite key:

> (`AUCTION_ID`, `SIZE`, `PRICE`)

```{r key-check-intial}
simple_unique <- ad_clean %>%
  distinct(AUCTION_ID, SIZE, PRICE) %>%
  nrow()

cat("Unique composite (AUCTION_ID, SIZE, PRICE): ", simple_unique)
```

The dataset contains `r nrow(ad_clean)` rows after duplicate removal,  
but only `r simple_unique` unique combinations of (`AUCTION_ID`, `SIZE`, `PRICE`).  
Thus, this composite key does **not** uniquely identify rows.

We create a helper function to allow us to search for a combination which will work as a key. 

**NOTE:** To avoid computational blow-up, we focus only on the most likely variables and limit our number of variables per composite key to 4:

```{r composite-key-helper}
check_candidate_keys <- function(data, cols, max_size = 4, show_full = TRUE) {
  results <- list()
  idx <- 1L
  
  for (k in seq_len(max_size)) {
    combos <- combn(cols, k, simplify = FALSE)
    
    for (combo in combos) {
      n_unique <- data %>%
        distinct(across(all_of(combo))) %>%
        nrow()
      
      results[[idx]] <- tibble(
        combo   = paste(combo, collapse = " + "),
        size    = k,
        n_rows  = nrow(data),
        n_unique = n_unique,
        is_key  = (n_unique == nrow(data))
      )
      idx <- idx + 1L
    }
  }
  
  results <- bind_rows(results)
  
  if (show_full) {
    # Full table (shown if needed for further exploration)
    results %>%
      arrange(desc(is_key), size, desc(n_unique))
  } else {
    # Compact summary
    results %>%
      group_by(size) %>%
      summarise(
        num_combos   = n(),
        num_keys     = sum(is_key),
        max_n_unique = max(n_unique),
        .groups = "drop"
      ) %>%
      arrange(size)
  }
}
```

We then use this function to check the most likely candidate variables:

```{r composite-key-search}
composite_candidates <- c(
  "AUCTION_ID",
  "SIZE",
  "PRICE",
  "RESPONSE_TIME",
  "BID_WON",
  "PUBLISHER_ID",
  "DEVICE_TYPE",
  "DEVICE_GEO_CITY",
  "DEVICE_GEO_ZIP",
  "DEVICE_GEO_LAT",
  "DEVICE_GEO_LONG",
  "TIMESTAMP"
)

key_results <- check_candidate_keys(
  data     = ad_clean,
  cols     = composite_candidates,
  max_size = 4,
  show_full = FALSE
)

key_results
```

From these results, we can see that no combination (up to four variables) of these candidate variables will create a composite key to uniquely identify the rows of this dataset.

---

## Summary

  - Exact duplicate rows were removed (`r n_dups` rows).
  - No reasonable variable or combination of variables uniquely identifies each row.  
  This is consistent with the data dictionary, which indicates that the true grain requires `bidder/seat`-level fields not present in this subset.

We proceed using **`ad_clean`** as our working dataset for variable-level cleaning.

---

---

# **VARIABLE**
   
## Description

  - Description & expected type/constraints (from dictionary).
   - Use the dictionary to define expectations.
   
## Exploration

  - values, distributions, weird codes, NA patterns.

## Cleaning

   -type conversion, recodes, outlier handling, bounds checks, etc.

## Summary

   -Add to cleaning_log if we changed anything.
   
---

---

# **CLEANED DATASET**

## Assemble `ad_clean`

## # Post-Cleaning Inspection

  - Final sanity checks on NA structure, bounds, duplicates

## Save via `data_io.R` function `export_ad_data()`

---

---

# **DATA LOG**

  - Display cleaning_log table.
  
  

  
---
---
---
---

  
  