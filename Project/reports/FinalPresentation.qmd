---
title: "STAT 531 — Final Presentation"
subtitle: "Oregon Ad Bidding: Data Cleaning, EDA, and Team Workflow"
author: "TECK Squad — Chris G., Takeshi S., Esther T., Khloud Z."
format:
  revealjs:
    slide-number: true
    incremental: true
    theme: beige
    transition: fade
    chalkboard: false
editor: visual
---

# Opening Slide

- **Project:** Oregon Ad Bidding Analysis
- **Team:** TECK Squad — *Chris G., Takeshi S., Esther T., Khloud Z.*
- **GitHub repository:** <https://github.com/etoobian/STAT531_Project/>
- **Summary:** A collaborative workflow demonstrating data cleaning, reproducibility, and exploratory analysis on Oregon advertising auction data.

::: notes
✅
Professor's requirement: 
- Project title 
- Team members 
- Repo link 
- One-sentence summary of project's focus
:::

------------------------------------------------------------------------

# Team Banner

## TECK Squad

- **T** — Takeshi
- **E** — Esther
- **C** — Chris
- **K** — Khloud

Previously known as **KTC (Kentucky Thighed Chicken)** before the team reformed.

::: notes
✅
Optional slide. Can add pic if we make one. (KTC Chicken?? New one??) Can be skipped if time is tight.
:::

------------------------------------------------------------------------

# The Data & Initial Plan (\< 5 min)

## Dataset Context

- Parquet dataset of Oregon digital ad auctions:
    - A prebid server handles online ad auctions where bidders submit CPM bids.
    - Individual bidders may bid on multiple sizes
    - Each row represents **a bid within a prebid auction**.
- Includes metadata such as:
    - Timestamps
    - Geolocation
    - Device Type

::: note
✅
Contains intentionally inserted data-quality issues
:::

## Guiding Questions
    
What findings can we deduce from:

- Checking usage of individual device types by city?
    
To help us deduce to what extent does:

- Device type impact response time?
- Geographic location impact response time?
- Response time impact the bid (price)?
    - By extension, geographic location impact bid (price)?

::: notes
✅✅✅✅
- What patterns appear across time, device type, and geography?
- How does missingness or inconsistency affect analysis?
- What behaviors or trends characterize Oregon's ad ecosystem?
:::

## Why These Questions Matter

These questions will help us by:

- Providing us insight to how geospatial data can be utilized to uncover potential issues regarding latency, which can improve overall bidding procedure.
- How to accommodate different device types to ensure fair bidding procedures.
- General understanding of how certain predictors influence each other will prove beneficial when looking to maximize bid prices and potentially reduce or standardize a maximum response time.

::: notes
✅✅✅✅
- Ethical concerns around targeting and bias
- Interpretability and decision-making
- Understanding data reliability and uncertainty
:::
::: notes
Professor's requirement (Section 2): 
- Basic data set context 
- Guiding questions we developed 
- Why these guiding questions matter
:::

------------------------------------------------------------------------

::: notes
Will be covered by Khloud.
:::

# Team Workflow & Roles

## Roles & Responsibilities

- **Chris** — EDA, Visualizer, Guiding Question Refinement
- **Takeshi** — Data Cleaner, Scribe, Presentation Editor.
- **Esther** — GitHub/Project Coordinator, Data Cleaner.
- **Khloud** — Data Cleaner, Presentation Editor.

## Coordination Tools

- **GitHub:** Checkout to Branch → PRs → Reviews → Merge.
- **Jira/Kanban:** Task/Subtask Alloting, Progress Tracking.
- **Discord:** Quick Updates, Online Meeting Location.
- **Commit Habits:** Consistent Commits, Detailed Messages.

Through the combination of Github, Jira, and a rigid commit/PR routine outlined by checklists/workflows that we agreed upon ensured we ran into minimal issues with the completion of this project.

::: notes
✅
Professor's requirement (Section 2): 
- The roles you assigned within the team and how you coordinated work: 
– GitHub workflow (branches, pull requests, commits) 
– Jira/Kanban tasks, deadlines, progress tracking 
- NOTE: Roles/coordination will be detailed in the next slide group Use this slide to emphasize collaboration practices, not code.

Refer to Team Docs for greater depth of detail while presenting?
:::

------------------------------------------------------------------------

# Data Cleaning Process (6-8 min)

## Key Issues Identified (Overview)

- Text and Value Inconsistencies.
- Implausible Values and Sentinels.
- Missing Information.
- Duplicates and Keys.

::: notes
✅
Professor's Section 3.1 — Key Issues You Found. Goal: show variety and importance of issues before deep-diving.

- Missingness patterns across multiple columns
- Text inconsistencies (typos, casing, spelling variants)
- Type mismatches (numbers stored as text, dates as strings)
- Duplicates and near-duplicates
- Outliers / implausible values (e.g., coordinates outside Oregon)
- Structural inconsistencies (unexpected categories, inconsistent naming)
:::

------------------------------------------------------------------------

::: notes
✅
Will be covered by Takeshi.
:::

# Issue 1: <br>Type and Value Inconsistencies

## Description

The type refers to the data type of a column and value refers to individual rows seen within the data set. Throughout the data set, we have encountered many inconsistencies, which for some, are distinguished using the data dictionary provided by the client.

## Examples

Examples of Type Inconsistencies:

- The PRICE column was of the character type.
- The RESPONSE_TIME column was of the character type.

Examples of Value Inconsistencies:

- The TIMESTAMP column has three different formats:
    - "YYYY/MM/DD...", "YYYY-MM-DD..." and "NA..."
- The RESPONSE_TIME column had different prefix delimeters:
    - "RESPONSE_TIME:" and "RESSPONSE_TIME:".

## Why It Matters

Issues pertaining to certain columns, like PRICE, were crucial to remedy because:

- These columns are crucial for exploring the guiding questions.
- Directly contradicted the data dictionary provided by the client.


## Cleaning Strategy

- Principle: Convert columns to their intended type and enforce sufficient formatting.
- Implementation: Compare types to data dictionary; remove delimiters; alleviate potential points of coercion; convert to correct data type.
- Example: Verify Data Type of PRICE → Identify Points of Coercion → Alleviate Given Points → Convert PRICE to the Numeric Data Type.

------------------------------------------------------------------------

::: notes
✅
Will be covered by Khloud.
:::

# Issue 2: <br>Implausible Values and Sentinels

## Description

Implausible values are values that otherwise, through the inherent existence of the respective column, can't exist. In most cases, sentinel values are also made to be implausible values, and therefore, there is also a need to distinguish between the two.

## Examples

Examples of Implausible Values:

- The PRICE column contains negative values.
- The DEVICE_GEO_LONG column contains values that exist outside of Oregon's border.

Sentinels:

- The generic sentinel value is -999, seen in columns such as:
    - PRICE
    - DEVICE_GEO_ZIP

## Why It Matters

Similar to the prior issue, with some additions:

- These columns are crucial for the exploration of the guiding questions.
- The inclusion of implausible and/or sentinel values can greatly skew any final conclusions.

## Cleaning Strategy (Fix Summary)

- Principle: Correct, if possible, or remove sentinels and implausible values.
- Implementation: Identify firm restrictions/bounds and impossible values; identify potential sentinel values; check respective values with bounds in mind; recover, if possible, or remove rows with implausible/sentinel values.
- Example: Verify that PRICE is Positive → Identify Negative Values → Delineate Between Sentinel and Implausible → Verify that Sentinel Values Can't be Recovered → Verify Distribution of Remaining Implausible Values to Test Recoverability → Recover Values.

------------------------------------------------------------------------

::: notes
✅
Will be covered by Khloud.
:::

# Issue 3: <br>Missing Values

## Description

Missing values refers to fields recorded as NA or left blank across one or more rows and/or columns. Furthermore, there is a need to distinguish if these missing values are a feature of the dataset or bad data.

## Examples 

Examples of bad data entries:

- The TIMESTAMPS column contains a format that leaves an NA in place of the date.
- The sentinel values seen in the PRICE column are irretrievable, and are no different to NA values.

## Why It Matters 

These NA values can directly impact the analysis of certain columns, and in the case that the data is retrievable, even more so, because:

- These NA values reduce the number of rows we can analyze.
- Creates potential bias if the present missingness is systematic.
- Limits our ability to answer key guiding questions. 

## Cleaning Strategy (Fix Summary) 

- Principle: Identify missing values and analyze for patterns and irretrievability.
- Implementation: Check for NAs; test the individual column impact on overall dataset; choose one of three options:
    - Drop the values.
    - Imputation.
    - Leave as is.
- Example: Identify PRICE Sentinels (NAs) → Determine Retrievability and Extent → Remove Sentinels.

---

# Issue 4: <br>Duplicates and Keys

## Description

Duplicate rows contain the exact same information for every single column. Keys pertain to the ability to uniquely identify rows given the least amount of unique columns.

## Why It Matters

Having duplicate data inflates the amount of rows, which can skew results.

\

It is intended for there to be a specific key given by the data dictionary, but is not present in the data.

## Cleaning Strategy (Fix Summary) 

- Principle: Remove rows with identical values in each column.
- Implementation: Remove duplicates.
- Example: Remove Duplicates by Taking Uniqueness.

::: notes 
✅
Professor's Section 3.2 — Cleaning Strategy: 
- What you observed 
- Why it was a problem 
- Principle behind your fix 
- Concise code or pseudo-code (keep it light)

Why It was Problem: 

- Misrepresents geographic locations
- Misleads EDA maps and summaries
- Could bias location-based conclusions

Cleaning Strategy: 

- Principle: enforce realistic Oregon bounds and correct shifted values
- Implementation: detect shifted observations; adjust values; re-check ranges
- Example (conceptual): mutate longitude values and validate them 
:::

::: notes
From Prof:
- For each issue: - What you observed - Why it was a problem - Principle behind your fix (e.g., “We chose this imputation strategy because...”) - Concise R code or pseudo-code Focus on justifying decisions, not on full code dumps."

Repeat this pattern for all Issues if needed, or combine smaller issues on a single slide.
:::

------------------------------------------------------------------------

# Reproducibility & Workflow

## Git + File Structure

- .../STAT531_Project/Project (root):
    - data/ (local, git-ignored)
    - scripts/ (modular R scripts)
    - notebooks/ (Rmd/knitted files)
    - results/ (saved plots)
- Snake case naming convention utilized (excluding team documents).
- Modularity achieved through the distinction of tasks.

## Documentation Practices

-   Quarto utilized for final presentation.
-   R Markdown for analysis steps and lab-style documentation.
-   Commit history shows frequent contributions from all team members.
-   Pull requestss used for review and discussion of content.

## Team Documents

- .../STAT531_Project/Project (root):
    - GitWorkflow.md 
    - PR_Checklist.md
    - TeamCharter.md
    - TeamCheckInEntries/ (check in folder)
    - TeamCheckInTeamplate.md

::: notes
✅
Professor's Section 3.3 — Reproducibility and Workflow: 
- Git usage (branches, PRs, conflicts) 
- Script modularity and readability 
- Rmd/Quarto 
- Naming conventions and folder structure
:::

------------------------------------------------------------------------

# Exploratory Data Analysis (7 - 10 min)

## Overview Patterns (likely split into multiple slides or infor below)

::: notes 
PLOTS:
- Distributions.
- Key Relationships.
- Missinginess profiles (from data_io.R)
- Surprising patterns.
:::

::::: columns
::: columns
### Figures (Placeholders)

-   Placeholder: Overall distributions figure
-   Placeholder: Key relationship plot
-   Placeholder: Missingness heatmap

(Example: save as reports/eda_overview_distributions.png,
figs/eda_overview_relationships.png, figs/eda_missingness.png)
:::

::: column
### Interpretation Notes

-   Describe main distribution shapes and any skew
-   Highlight major relationships (e.g., device vs outcome)
-   Summarize missingness profile and any surprising gaps
-   Note any patterns that motivated deeper guiding questions
:::
:::::

::: notes
EDA Overview 
- High-level distributions 
- Relationships 
- Missingness summaries 
Professor's Sections 4.1 and 4.3: 
- Show distributions, relationships, missingness, and surprising patterns 
- Ensure clear labels, good color choices, no clutter and interpretation w/ each visual This slide uses side-by-side layout for plots and narrative.
:::

------------------------------------------------------------------------

# Deep Dive: Guiding Question 1

## Question and Visuals

::::: columns
::: column
### Guiding Question 1 (example)

-   State the question clearly
    (for example: "How does device type vary by time of day?")

#### Figure Placeholder

-   Insert main plot here (e.g., time-of-day vs device type)
-   File suggestion: figs/gq1_main_plot.png
:::

::: column
### Interpretation

-   Describe the pattern shown in the figure
-   Connect back to the guiding question explicitly
-   Mention how cleaning decisions (e.g., device-type standardization) were necessary for this figure to be meaningful
-   Note any caveats or surprising findings
:::
:::::

::: notes
Professor's Section 4.2 — Deep Dives on Guiding Questions: 
- State the question 
- Show relevant figures 
- Interpret results 
- Connect to earlier cleaning decisions
:::

------------------------------------------------------------------------

# Deep Dive: Guiding Question 2

## Question and Visuals

::::: columns
::: column
### Guiding Question 2

-   State the question clearly
    (for example: "Are ad impressions concentrated in particular regions of Oregon?")

#### Figure Placeholder

-   Insert main map or spatial plot here
-   File suggestion: figs/gq2_map_plot.png
:::

::: column
### Interpretation

-   Summarize spatial concentration or lack thereof
-   Discuss how coordinate cleaning affected conclusions
-   Note any hotspots, gaps, or anomalies
-   Comment briefly on ethical or fairness implications if relevant
:::
:::::

::: notes
For each: 
- State the question 
- Show relevant figures 
- Interpret results clearly 
- Connect to cleaning decisions

Same structure as Guiding Question 1, maintaining consistent layout: 
plots on left, interpretation on right.
:::

------------------------------------------------------------------------

# Key Findings (2-3 min)

## Summary of Insights

-   3–5 major findings from the cleaned and explored data
-   One key insight about data quality and cleaning impact
-   One insight about behavior or patterns in the ad ecosystem
-   One insight about limitations or uncertainties in the data

::: notes
Professor's Section 5 — Insight Summary: 
- Summarize 3-5 most important / key findings 
- Explain what the data suggests overall 
- Mention uncertainties or next steps Keep bullets crisp and non-technical.
:::

------------------------------------------------------------------------

# Collaboration Reflection (2-3 min)

## What Went Well

-   Effective use of GitHub, branches, and PRs.
-   Clear division of labor and ownership.
-   Helpful use of Jira/Kanban for tracking progress.

## Challenges and Lessons

-   Merge conflicts and how they were resolved.
-   Aligning on cleaning decisions and documentation style.
-   Time management and coordination across schedules.

::: notes
Professor's Section 6 — Collaboration Reflection: 
- What went wellin workflow 
- What was challenging 
- Lessons learned for future projects 
- Role of GitHub, Jira, RStudio in collaboration
:::

------------------------------------------------------------------------

# Final Slide: Repository Checklist (1 min)

## GitHub Repo Contents

-   Clean and informative README
-   Data-cleaning scripts
-   EDA notebooks (Rmd and/or qmd)
-   Clear folder structure: data, scripts, figs, docs
-   Kanban board snapshot (exported image or link)
-   Evidence of teamwork: PRs, commit history, multiple authors

::: notes
Professor's Section 7 — GitHub Repository Checklist. 
Confirm your repo includes: -
Clean README 
- Data cleaning scripts 
- EDA scripts (.qmd or .Rmd) 
- Clear folder structure (data/, R/, figs/) 
- Kanban snapshot 
- Evidence of teamwork (commit history, pull requests, etc.)
:::

::: notes
Presentation Logistics Timing 
- Total: 20–30 minutes 
- Leave 3 min for questions 
- Practice pacing
:::
